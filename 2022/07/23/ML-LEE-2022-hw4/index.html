<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha256-xejo6yLi6vGtAjcMIsY8BHdKsLg7QynVlFMzdQgUuy8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"laxiflora.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.12.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"${hits} 找到 ${time} 個結果","hits":"找到 ${hits} 個結果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="作業題目 可進行的目標  Medium : 0.70375 (train 1~1.5 hr) Strong : 0.77750 (train 3~4 hr) Boss : 0.86500 (train 2~2.5 hr )  調整transformer參數與pred_layer模型 包括降低multi-head數量、feedforward dropout rate等等 降低pred-layer的l">
<meta property="og:type" content="article">
<meta property="og:title" content="ML_LEE_2022_hw4">
<meta property="og:url" content="https://laxiflora.github.io/2022/07/23/ML-LEE-2022-hw4/index.html">
<meta property="og:site_name" content="laxiflora的小天地">
<meta property="og:description" content="作業題目 可進行的目標  Medium : 0.70375 (train 1~1.5 hr) Strong : 0.77750 (train 3~4 hr) Boss : 0.86500 (train 2~2.5 hr )  調整transformer參數與pred_layer模型 包括降低multi-head數量、feedforward dropout rate等等 降低pred-layer的l">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://laxiflora.github.io/images/20220723_2.png">
<meta property="og:image" content="https://laxiflora.github.io/images/20220722_1.png">
<meta property="og:image" content="https://laxiflora.github.io/images/20220722_2.png">
<meta property="og:image" content="https://laxiflora.github.io/images/20220723_1.png">
<meta property="og:image" content="https://laxiflora.github.io/images/20220723_4.png">
<meta property="og:image" content="https://laxiflora.github.io/images/20220723_7.png">
<meta property="og:image" content="https://laxiflora.github.io/images/20220723_5.png">
<meta property="og:image" content="https://laxiflora.github.io/images/20220723_6.png">
<meta property="og:image" content="https://laxiflora.github.io/images/pasted-23.png">
<meta property="article:published_time" content="2022-07-22T17:12:30.000Z">
<meta property="article:modified_time" content="2022-08-08T12:10:07.962Z">
<meta property="article:author" content="劉宇承">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://laxiflora.github.io/images/20220723_2.png">


<link rel="canonical" href="https://laxiflora.github.io/2022/07/23/ML-LEE-2022-hw4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-TW","comments":true,"permalink":"https://laxiflora.github.io/2022/07/23/ML-LEE-2022-hw4/","path":"2022/07/23/ML-LEE-2022-hw4/","title":"ML_LEE_2022_hw4"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ML_LEE_2022_hw4 | laxiflora的小天地</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">laxiflora的小天地</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">前進軟體工程師的練功之路</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>歸檔</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%AF%E9%80%B2%E8%A1%8C%E7%9A%84%E7%9B%AE%E6%A8%99"><span class="nav-number">1.</span> <span class="nav-text">可進行的目標</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AA%BF%E6%95%B4transformer%E5%8F%83%E6%95%B8%E8%88%87pred-layer%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.</span> <span class="nav-text">調整transformer參數與pred_layer模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%A6%E4%BD%9Cconformer"><span class="nav-number">1.2.</span> <span class="nav-text">實作conformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9B%B4%E5%8B%95transformer-x2F-comformer-encode%E5%B1%A4%E6%95%B8"><span class="nav-number">1.3.</span> <span class="nav-text">更動transformer&#x2F;comformer encode層數</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AF%AB%E4%BD%9C%E6%A5%AD%E6%AD%B7%E7%A8%8B"><span class="nav-number">2.</span> <span class="nav-text">寫作業歷程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2022-x2F-7-x2F-22"><span class="nav-number">2.1.</span> <span class="nav-text">2022&#x2F;7&#x2F;22</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2022-x2F-7-x2F-23"><span class="nav-number">2.2.</span> <span class="nav-text">2022&#x2F;7&#x2F;23</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%9C%E6%A5%AD%E7%B5%90%E6%9E%9C"><span class="nav-number">3.</span> <span class="nav-text">作業結果</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Report"><span class="nav-number">4.</span> <span class="nav-text">Report</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">劉宇承</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="回到頂端">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-TW">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/23/ML-LEE-2022-hw4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ML_LEE_2022_hw4 | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ML_LEE_2022_hw4
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-23 01:12:30" itemprop="dateCreated datePublished" datetime="2022-07-23T01:12:30+08:00">2022-07-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/ml2022spring-hw4">作業題目</a></p>
<h1 id="可進行的目標"><a href="#可進行的目標" class="headerlink" title="可進行的目標"></a>可進行的目標</h1><p><img src="/../images/20220723_2.png"></p>
<ul>
<li>Medium : 0.70375 (train 1~1.5 hr)</li>
<li>Strong : 0.77750 (train 3~4 hr)</li>
<li>Boss : 0.86500 (train 2~2.5 hr )</li>
</ul>
<h2 id="調整transformer參數與pred-layer模型"><a href="#調整transformer參數與pred-layer模型" class="headerlink" title="調整transformer參數與pred_layer模型"></a>調整transformer參數與pred_layer模型</h2><ul>
<li>包括降低multi-head數量、feedforward dropout rate等等</li>
<li>降低pred-layer的linear connect layer層數 （這個問題複雜度不高，線性聯階層不用到兩層）</li>
</ul>
<h2 id="實作conformer"><a href="#實作conformer" class="headerlink" title="實作conformer"></a>實作conformer</h2><ul>
<li>把原本的 <code>self.encoder_layer</code>改掉，套用conformer模型</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.08100">conformer來源</a></li>
<li>記得修改Classifier的forward</li>
</ul>
<h2 id="更動transformer-x2F-comformer-encode層數"><a href="#更動transformer-x2F-comformer-encode層數" class="headerlink" title="更動transformer&#x2F;comformer encode層數"></a>更動transformer&#x2F;comformer encode層數</h2><ul>
<li>原本只有一層(就是<code>self.encoder_layer</code>)，可以改成呼叫<code>self.encoder</code>並修改Classifier的forward</li>
</ul>
<hr>
<h1 id="寫作業歷程"><a href="#寫作業歷程" class="headerlink" title="寫作業歷程"></a>寫作業歷程</h1><h2 id="2022-x2F-7-x2F-22"><a href="#2022-x2F-7-x2F-22" class="headerlink" title="2022&#x2F;7&#x2F;22"></a>2022&#x2F;7&#x2F;22</h2><p>這次作業transformer（self-attention）在NLP與語音辨識領域運用廣泛，是我想要好好學好的章節，不過這次作業給我挫敗感很大…<br>這份作業寫一整天，白天的時候在查<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.08100">comformer的paper</a>，但是因為對於transformer的觀念也不甚熟悉，所以照著李宏毅老師給的reference查到了<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/263031249">這篇paper（中文心得版）</a>，在大略的瞭解了transformer家族以後，最後回去看了一下<a target="_blank" rel="noopener" href="https://medium.com/ching-i/transformer-attention-is-all-you-need-c7967f38af14">別人看「Attention is all you need」的心得</a>，對照上課的筆記這才更有了概念</p>
<p>到了晚上終於要開始著手改code，首先在<a target="_blank" rel="noopener" href="https://pytorch.org/audio/stable/_modules/torchaudio/models/conformer.html#Conformer">torchaudio document</a>查到了他支援的conformer model API，開始套用以後才開始發現他的問題：看似簡單的forward修改其實困難重重！</p>
<p>首先是最基礎的問題，因為是pytorch菜雞，我甚至連forward的意涵是什麼都不知道，後來查到原來他是__call__會呼叫的函數，通常進行一個step（單位為一batch）的計算，而這份code在訓練時會呼叫model_fn</p>
<p>model_fn用意在於<br>    - 先做mels與labels的分類<br>    - 把資料放到GPU上<br>    - 呼叫<code>model.forward()</code>以及計算loss<br>    - 最後進行predict以後算出該step之acc</p>
<p>搞懂了forward以後，開始著手修改forward內部的code使他貼合conformer，但因為這樣所以我也必須去查torchaudio.models.Conformer這個class裡面的forward會怎麼運作（巢狀模型），查到的資料如下：<br><img src="/../images/20220722_1.png"><br>註解寫到lengths with shape(B,)，因此我很直觀的直接打了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># out: (batch size, length, d_model)</span><br><span class="line">self.encoder(out,(out[0],) ) </span><br></pre></td></tr></table></figure>
<p>這麼做跳了一個錯誤：他抓不到<code>out[0]</code>的shape，我這才發現他要的是torch.Tensor type，於是我改成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.encoder(out, torch.tensor(out[0],) )</span><br></pre></td></tr></table></figure>
<p>仔細研究torch.tensor用法以後終於可以正確地塞tensor進去，但是我卻鬼打牆的一直卡在lengths這個參數的shape不對，從錯誤碼發現跟ket_padding_mask有關，幾經波折以後看到<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/353365423">這篇</a>才發現他要吃的參數是(batch size,sequence length)，最後修正成了這樣</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out = self.encoder(out,torch.tensor([out[0],out[1]]))</span><br></pre></td></tr></table></figure>
<p>卻碰到了新的錯誤碼：<br><img src="/../images/20220722_2.png"><br>留待明天處理…</p>
<hr>
<h2 id="2022-x2F-7-x2F-23"><a href="#2022-x2F-7-x2F-23" class="headerlink" title="2022&#x2F;7&#x2F;23"></a>2022&#x2F;7&#x2F;23</h2><p>果然是昨天腦袋昏到不好思考，把out.size(0)跟out[0].size()搞混，雖說後來修正了，但還是套不進去。對於tensor dim的掌握力太差了…<br><img src="/../images/20220723_1.png"></p>
<p>後來改用<a target="_blank" rel="noopener" href="https://github.com/lucidrains/conformer">這個conformer</a>來實作，多了很多我不知道幹嘛用的參數，而且看似不能增加layer數orz…，先求能跑吧，感覺達不到strong base line了，參數如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">self.encoder_layer = ConformerBlock(</span><br><span class="line">    dim = d_model,</span><br><span class="line">    dim_head = 64,</span><br><span class="line">    heads = 2,</span><br><span class="line">    ff_mult = 4,</span><br><span class="line">    conv_expansion_factor = 2,</span><br><span class="line">    conv_kernel_size = 31,</span><br><span class="line">    attn_dropout = 0.,</span><br><span class="line">    ff_dropout = 0.,</span><br><span class="line">    conv_dropout = 0.</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>要注意的是他的forward吃的參數是<code>(length, batch size, d_model)</code>，所以permute那行註解要拿掉，這方面沒有統一真的很煩人，最後Classifier forward code如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, mels):</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	args:</span><br><span class="line">		mels: (batch size, length, 40)</span><br><span class="line">	return:</span><br><span class="line">		out: (batch size, n_spks)</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	# out: (batch size, length, d_model)</span><br><span class="line">	out = self.prenet(mels)</span><br><span class="line">	# out: (length, batch size, d_model)</span><br><span class="line">	out = out.permute(1, 0, 2)</span><br><span class="line">	# The encoder layer expect features in the shape of (length, batch size, d_model).</span><br><span class="line"> #out = self.encoder_layer(out)</span><br><span class="line">  # Do 2 conformer layer</span><br><span class="line">#	out = self.encoder_layer(out)</span><br><span class="line">	out = self.encoder_layer(out)</span><br><span class="line">	# out: (batch size, length, d_model)</span><br><span class="line">	out = out.transpose(0, 1)</span><br><span class="line">	# mean pooling</span><br><span class="line">	stats = out.mean(dim=1)</span><br><span class="line"></span><br><span class="line">	# out: (batch, n_spks)</span><br><span class="line">	out = self.pred_layer(stats)</span><br><span class="line">	return out</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>中間有個<code>out = self.encoder_layer(out)</code>被註解了，我原本是想說既然參數不能設定layer數，那就讓他跑兩次也可以是兩輪，不過我想太美了，因為GPU多線程的關係，讓整個tqdm的log大暴走，正確性也打個問號，搞不好會碰到race condition…，因為這是這個模型設計者沒設計到的部分(或是我菜到沒發現QQ)，所以最後決定讓他就單層去跑</p>
<hr>
<h1 id="作業結果"><a href="#作業結果" class="headerlink" title="作業結果"></a>作業結果</h1><p>Train了70000 step (大約1 hr)，跟strong差一點點，但acc還在上升，感覺是練不夠久<br><img src="/../images/20220723_4.png"><br>補了以下的code讓他繼續撿剛剛的模型train</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if exists(&quot;./model.ckpt&quot;):</span><br><span class="line">	model.load_state_dict(torch.load(&quot;./model.ckpt&quot;))</span><br><span class="line">	print(&quot;Exist model detected, continue training...&quot;)</span><br><span class="line">       if model.training() == False:</span><br><span class="line">		model.train()</span><br></pre></td></tr></table></figure>
<p>得到命中率&#x3D;79%<br><img src="/../images/20220723_7.png"></p>
<p>但提交的時候卻出了大問題，測試的結果非常差<br><img src="/../images/20220723_5.png"><br>有查到一篇心得(下圖)也是成績暴跌，推測測試集跟訓練集的分布差很多(不過那篇作者是碰到切割長度的問題，這我沒有改不該有影響才對)，這份作業做到這吧，沒力氣修正了orz，再修也會淪為調參之流，感覺學不到甚麼。<br>結論姑且下在data mismatch，不過模型參數與notebook我會保留在github，將來靈光乍現的時候再來看看</p>
<p><img src="/../images/20220723_6.png"></p>
<h1 id="Report"><a href="#Report" class="headerlink" title="Report"></a>Report</h1><p><img src="/../images/pasted-23.png"></p>
<ol>
<li><p>因為卡住所以我有多看幾個varient</p>
<ul>
<li>linformer : 藉由attention matrix在做sorftmax轉化以後非行滿秩的特性，降低attention matrix的維度，使得transformer的複雜度可以降到線姓<br>    - Sparse Attention (FP&#x2F;LP) : 因為傳統transformer的複雜度過高，所以藉由讓self-attention關注的範圍由全局變成局部性，降低計算時間。FP(fixed patterns)是固定self-attention窗口大小，LP則是把它的大小變成一個可學習的參數</li>
</ul>
</li>
<li><p>因為transformer雖然可以考慮sequence的全局性，但對於自己的重視程度較為有限，而CNN則可以對局部性的特徵著重關注，兩者混合起來可以達到互補提高準確性的效果。</p>
</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/07/22/Efficient-Transformers-A-Survey%E7%AD%86%E8%A8%98/" rel="prev" title="Efficient Transformers:A Survey筆記">
                  <i class="fa fa-chevron-left"></i> Efficient Transformers:A Survey筆記
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/07/23/ML-2021-5-2-Transformer%EF%BC%88%E4%B8%8A%EF%BC%89/" rel="next" title="ML_2021_5-2 Transformer（上）">
                  ML_2021_5-2 Transformer（上） <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">劉宇承</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 強力驅動
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha256-xejo6yLi6vGtAjcMIsY8BHdKsLg7QynVlFMzdQgUuy8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"laxiflora.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.12.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"${hits} 找到 ${time} 個結果","hits":"找到 ${hits} 個結果"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="laxiflora的小天地">
<meta property="og:url" content="https://laxiflora.github.io/page/2/index.html">
<meta property="og:site_name" content="laxiflora的小天地">
<meta property="og:locale" content="zh_TW">
<meta property="article:author" content="劉宇承">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://laxiflora.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-TW","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>laxiflora的小天地</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">laxiflora的小天地</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">前進軟體工程師的練功之路</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>歸檔</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">劉宇承</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="回到頂端">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/28/ML-2021-6-2-%E7%94%9F%E6%88%90%E5%BC%8F%E5%B0%8D%E6%8A%97%E7%B6%B2%E8%B7%AF-%E4%BA%8C-%E7%90%86%E8%AB%96%E4%BB%8B%E7%B4%B9%E8%88%87WGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/28/ML-2021-6-2-%E7%94%9F%E6%88%90%E5%BC%8F%E5%B0%8D%E6%8A%97%E7%B6%B2%E8%B7%AF-%E4%BA%8C-%E7%90%86%E8%AB%96%E4%BB%8B%E7%B4%B9%E8%88%87WGAN/" class="post-title-link" itemprop="url">ML_2021_6-2 生成式對抗網路(二) - 理論介紹與WGAN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-28 00:53:06" itemprop="dateCreated datePublished" datetime="2022-07-28T00:53:06+08:00">2022-07-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="訓練的目標"><a href="#訓練的目標" class="headerlink" title="訓練的目標"></a>訓練的目標</h1><ul>
<li>訓練到底要min&#x2F;max甚麼東西呢?<br><img src="/../images/20220728_1.png"></li>
</ul>
<p>一維的範例</p>
<p><img src="/../images/20220728_2.png"></p>
<ul>
<li><p>要找的最佳化G參數就是<br>$$<br>G^* &#x3D; arg\ min_GDiv(P_G,P_{data})<br>$$</p>
</li>
<li><p>其中Div()表示兩個distribution之間的距離(相似度)公式</p>
</li>
<li><p>問題在不知道怎麼計算divergence</p>
</li>
<li><p>GAN可以在只有Sample的情況下，估計出div()是多少</p>
<ul>
<li>需要從$P_G、P_{data}$取樣，$P_{data}$取自圖庫，而$P_G$則取自generator產生的圖片<br>    - 這部分就要交給Discriminator，他要max一個objective function，公式有很多種</li>
</ul>
</li>
</ul>
<h2 id="JS-divergence"><a href="#JS-divergence" class="headerlink" title="JS divergence"></a>JS divergence</h2><p>公式如下(我們要取Max)<br>    $$<br>    V(G,D) &#x3D; E_{y\ from\ P_{data}}[logD(y)] + E_{y\ from\ P_G}[log(1-D(y)]<br>    $$</p>
<ul>
<li>我們會需要來自data的D(y)越大越好，來自G的D(y)越小越好</li>
</ul>
<p>Note: 若加上一點自由度，簡化上述公式，可以得到<br>$$<br>J^D &#x3D; -D(x) + D(G(z)), for\ all\ D(x),\ D(G(z))\  \in \ [0,1]<br>$$<br>且生成器的損失函數：<br>$$<br>J^G &#x3D; -J^D<br>$$<br>因為他們之間彼此對抗，所以他們兩者之間的損失只差一個負號，稱為min-max GAN</p>
<p><img src="/../images/20220729_1.png"></p>
<ul>
<li>其實 $D^*$ 等同於(-1) x cross entropy<ul>
<li>早年這麼設計的理由是因為，希望objective function可以跟二元分類扯上關係</li>
</ul>
</li>
</ul>
<p><img src="/../images/20220728_3.png"></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.2661">參考</a></p>
</li>
<li><p>small divergence，data跟G的圖很像，則hard to discrininate，則small max V(D,G)</p>
</li>
<li><p>因為$max_D\ V(D,G)$與JS divergence有關聯，所以我們可以把Div()換掉，變成<br>$$<br>G^* &#x3D; arg\ min_G [max_DV(G,D)] \\\<br>D^* &#x3D; arg\ max_DV(D,G)<br>$$<br>&#x2F;&#x2F;D的max objetive value跟JS divergence有關</p>
</li>
</ul>
<h2 id="其他的divergence"><a href="#其他的divergence" class="headerlink" title="其他的divergence"></a>其他的divergence</h2><ul>
<li>當然，我們也可以用不同的divergence<ul>
<li>對於不同的divergence，用甚麼樣的objective function，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.00709">這篇論文有詳細解釋</a><br><img src="/../images/20220728_4.png"></li>
</ul>
</li>
</ul>
<h1 id="訓練GAN的小技巧"><a href="#訓練GAN的小技巧" class="headerlink" title="訓練GAN的小技巧"></a>訓練GAN的小技巧</h1><h2 id="JS-divergence-的問題"><a href="#JS-divergence-的問題" class="headerlink" title="JS divergence 的問題"></a>JS divergence 的問題</h2><ul>
<li><p>$P_G、P_{data}$重疊的地方往往很少</p>
<ul>
<li>pf1. 圖片是高維空間裡面，低維的manifold<br>    	- 就像在一個平面空間中的兩條線一樣，重合的地方很少<br>    	- 在高維空間內隨便sample的點都不會是圖片<br>    	- 所以他們相交的部分幾乎可以忽略</li>
<li>pf2.若$P_G、P_{data}$sample的點不夠多，很容易劃出一個界線把他們切開<br>-&gt; $P_G、P_{data}$重疊範圍非常少</li>
</ul>
</li>
<li><p>若兩個分布沒有重疊的地方，算出來的Div就會永遠都是log 2，看不出差距<br><img src="/../images/20220728_5.png"></p>
</li>
</ul>
<h2 id="WGAN"><a href="#WGAN" class="headerlink" title="WGAN"></a>WGAN</h2><ul>
<li>換一個衡量divergense來衡量2 distribution之相似度</li>
</ul>
<h3 id="Wasserstein-distance"><a href="#Wasserstein-distance" class="headerlink" title="Wasserstein distance"></a>Wasserstein distance</h3><ul>
<li><p>假設一個distribution P為一坨土，而另一個distribution Q為目的地</p>
</li>
<li><p>把土堆P挪到Q所需要的移動距離平均就是Wasserstein distance<br><img src="/../images/pasted-34.png"></p>
</li>
<li><p>因為可能的挪法很多，所以d會有不同</p>
<ul>
<li>定義: 窮舉所有的moving plan，找出最小的移動距離當作wasserstein distance<ul>
<li>計算麻煩</li>
</ul>
</li>
</ul>
</li>
<li><p>假設我們能計算Wasserstein distance，帶來的優點:</p>
<ul>
<li>就可以解決JS divergence看不出上圖的好壞比較的問題  <br><img src="/../images/20220728_7.png"></li>
</ul>
</li>
</ul>
<h3 id="Evaluate-Wasserstein-distance"><a href="#Evaluate-Wasserstein-distance" class="headerlink" title="Evaluate Wasserstein distance"></a>Evaluate Wasserstein distance</h3><ul>
<li>解下面的Optimization問題(下圖)，解出來就會是Wasserstein distance<br><img src="/../images/20220728_9.png"><br>[、]是期望值，D(x)就是剛剛的D(y)</li>
<li>D必須是1-Lipschitz function (Discriminator不可變化劇烈)<ul>
<li>如果沒有這個constraint，則D的training不會收斂<br>    - 讓D保持smooth強迫D(x)變成無窮與負無窮</li>
<li>基本上就是保證real跟generated的data距離不會太遠</li>
</ul>
</li>
</ul>
<p><img src="/../images/pasted-35.png"></p>
<h3 id="how-to-確保這個式子可用"><a href="#how-to-確保這個式子可用" class="headerlink" title="how to 確保這個式子可用"></a>how to 確保這個式子可用</h3><ul>
<li><p>原始GAN方法</p>
<ul>
<li>強迫network的parameters w bound在[c,-c]<br>    - 在梯度下降的para更新後，若w&gt;c , w&#x3D;c ; if w&lt;-c , w &#x3D; -c<br>    - 可能可以讓function平滑一點，但沒有解決問題</li>
</ul>
</li>
<li><p>有一篇 paper : improved WGAN 做的處理方法:</p>
<ul>
<li>在real data取sample，在fake data取一個sample，在中間再取一個sample，這個sample的梯度需要接近1 (?)</li>
</ul>
</li>
</ul>
<p>      <br><img src="/../images/pasted-36.png"></p>
<ul>
<li>相關方法很多，可以多查查<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05957">Spectral Norm</a><ul>
<li>Keep gradient norm在哪都 &lt; 1</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q &amp; A"></a>Q &amp; A</h4><p>Q1: 在discriminator訓練時，可否加入GAN以往的輸出</p>
<p>A1: 可。實務上跑的時候不會真的讓discriminator被maximize，太花時間，所以通常幾個iteration後就會轉換到generator</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/27/ML-2021-6-1-%E7%94%9F%E6%88%90%E5%BC%8F%E5%B0%8D%E6%8A%97%E7%B6%B2%E8%B7%AF-%E4%B8%80-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%B4%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/27/ML-2021-6-1-%E7%94%9F%E6%88%90%E5%BC%8F%E5%B0%8D%E6%8A%97%E7%B6%B2%E8%B7%AF-%E4%B8%80-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%B4%B9/" class="post-title-link" itemprop="url">ML_2021_6-1 生成式對抗網路(一) - 基本概念介紹</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-27 22:52:40" itemprop="dateCreated datePublished" datetime="2022-07-27T22:52:40+08:00">2022-07-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><img src="/../images/20220727_4.png"></p>
<ul>
<li>到目前為止的network都是一個function</li>
<li>這次則是把network當作generator使用<ul>
<li>現在的network會加入一個new variable z，現在network就有兩筆輸入了</li>
</ul>
</li>
<li>z由某個simple distribution生成(distribution必須夠簡單 ex. uniform)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.2661">初代GAN</a></li>
</ul>
<h1 id="Why-需要輸出是一個分布"><a href="#Why-需要輸出是一個分布" class="headerlink" title="Why 需要輸出是一個分布"></a>Why 需要輸出是一個分布</h1><ul>
<li>以video prediction「小精靈」為例說明<ul>
<li>給定previous frames，輸出預測下一個frame會出現的畫面</li>
<li>但是在類似畫面下，有時候小精靈往左轉，有時候他往右轉，導致分裂(機器選擇兩面討好，這樣對兩個case的loss最小)<br>    -&gt; 讓機器的輸出不再單一，而是一個機率性的分佈  <br><img src="/../images/pasted-32.png"></li>
</ul>
</li>
</ul>
<p>    </p>
<p>在機器的輸出需要創造性的時候(同一輸入可能有多種輸出)，就會需要distribution<br>    - ex. 繪圖、寫文章、對話</p>
<h1 id="Generative-Adversarial-Network-GAN"><a href="#Generative-Adversarial-Network-GAN" class="headerlink" title="Generative Adversarial Network(GAN)"></a>Generative Adversarial Network(GAN)</h1><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/hindupuravinash/the-gan-zoo">有很多種變形</a></li>
</ul>
<h2 id="Anime-face-ganeration"><a href="#Anime-face-ganeration" class="headerlink" title="Anime face ganeration"></a>Anime face ganeration</h2><ul>
<li>先把x輸入拿掉，並假設z輸入是normal distribution</li>
<li>其實二次元人物的臉(圖片)就只是一個高維向量</li>
<li>輸入的distribution複雜性其實影響不大，因generator會想辦法把它變複雜</li>
</ul>
<h2 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h2><ul>
<li>在GAN中會有這樣一個神經網路，用於判別輸出的東西是否真實</li>
<li>輸入是產生的圖片，而輸出則是一種純量[0.1]</li>
<li>可用CNN、transformer…等都可以</li>
</ul>
<h2 id="Basic-Idea-of-GAN"><a href="#Basic-Idea-of-GAN" class="headerlink" title="Basic Idea of GAN"></a>Basic Idea of GAN</h2><p>示意圖:<br><img src="/../images/20220728_6.png"></p>
<ul>
<li><p>就像物競天擇的演化一樣，discriminator會去除掉得分低的GAN成品，而GAN成品就會「天擇」成discriminator比較能接受的情況</p>
</li>
<li><p>但是discriminator也會進化，讓GAN必須再繼續進行進化</p>
</li>
<li><p>discriminator與generator通常互相視為敵人<br>GAN版示意圖(毛圖注意)<br><img src="/../images/20220727_8.png"></p>
</li>
</ul>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><ol>
<li><p>Init gen and discriminator</p>
</li>
<li><p>固定住Generator (G)，輸入一陀random sampled vector，吐出成品，更新discriminator (D)</p>
<ul>
<li>拿一些「ground truth」 (ex. 真正的二次元人物)，互相比較相似度，並且更新D</li>
<li>對D來說這是一個分類問題，或是regression問題 (正確圖片標1，錯的標0)，總之就是看個人想怎麼做</li>
</ul>
</li>
<li><p>固定D，更新G，G要練習把D的acc升到最高(去欺騙D)</p>
<ul>
<li>把兩個neural network接起來，就會變成「輸入是一個向量，而輸出會是一個分數」，所以現在情況等同於maximize score (gradient ascent)<br>    - 當然固定D，所以「評分系統」不可變動</li>
</ul>
</li>
<li><p>LOOP，反覆訓練G、D<br><img src="/../images/pasted-33.png"></p>
</li>
</ol>
<hr>
<h2 id="現代GAN"><a href="#現代GAN" class="headerlink" title="現代GAN"></a>現代GAN</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.gwern.net/Faces">StyleGAN</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.10196">Progressive GAN</a></p>
</li>
<li><p>GAN可以產生沒看過的人臉，做一些內插<br><img src="/../images/20220728_10.png"></p>
</li>
<li><p>下一堂講解theory behind GAN</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/24/ML-2021-5-3-Transformer%EF%BC%88%E4%B8%8B%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/24/ML-2021-5-3-Transformer%EF%BC%88%E4%B8%8B%EF%BC%89/" class="post-title-link" itemprop="url">ML_2021_5-3 Transformer（下）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-24 22:54:31" itemprop="dateCreated datePublished" datetime="2022-07-24T22:54:31+08:00">2022-07-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>接續5-2，講論文的transformer中的decoder區塊</li>
<li>Decoder有兩種<ul>
<li>autoregressive(AT)</li>
<li>non-autoregressive(NAT)</li>
</ul>
</li>
<li>這堂課主講AT</li>
</ul>
<h1 id="Transformer：Decoder的運作-語音辨識為範例"><a href="#Transformer：Decoder的運作-語音辨識為範例" class="headerlink" title="Transformer：Decoder的運作 (語音辨識為範例)"></a>Transformer：Decoder的運作 (語音辨識為範例)</h1><ul>
<li><p>輸入一段聲音，輸出一段文字</p>
</li>
<li><p>encoder收到聲音訊號(seq)以後，輸出一排vector(seq)</p>
</li>
<li><p>Decoder用於產生語音辨識的結果<br><img src="/../images/20220726_1.png"></p>
</li>
<li><p>輸出會是一個向量(one-hot vector)，他的長度就是整個語料庫的大小</p>
<ul>
<li>中文通常就是一個字為單位，英文有可能以詞為單位，也可能以字根字首為單位(subwords)</li>
</ul>
</li>
<li><p>第一步驟的時候，decoder會輸入一個token(BEGIN)，一種特殊符號</p>
</li>
<li><p>接下來第二及之後的步驟，會把前面步驟輸出的字母丟入decoder作為輸入<br><img src="/../images/20220726_2.png"></p>
<ul>
<li>這樣的問題就是，如果前面辨識產生的結果是錯的，那後面的輸出也會受到錯誤的結果影響 (error propagation)</li>
</ul>
</li>
</ul>
<h1 id="Transformer：Decoder結構"><a href="#Transformer：Decoder結構" class="headerlink" title="Transformer：Decoder結構"></a>Transformer：Decoder結構</h1><ul>
<li>暫且忽略來自encoder的輸入</li>
</ul>
<p><img src="/../images/20220726_3.png" alt="upload successful"></p>
<ul>
<li>撇除圈起來的地方不看，可以發現decoder跟encoder的差別沒有很大</li>
</ul>
<h2 id="masked-self-attention"><a href="#masked-self-attention" class="headerlink" title="masked self-attention"></a>masked self-attention</h2><ul>
<li>注意到decoder第一格有一個”masked multi-head attention”</li>
<li>相對於原始的self-attention，decoder只能參考前面的vector，不能參考之後的<br><img src="/../images/20220726_4.png"></li>
<li>因為token是一個一個產生，不能考慮右邊</li>
</ul>
<h2 id="Determine-output-length"><a href="#Determine-output-length" class="headerlink" title="Determine output length"></a>Determine output length</h2><ul>
<li>目前的decoder運作機制，不知道甚麼時候該停下來 (無限自動選字的概念)</li>
<li>需要有一個「斷」的token，塞在輸出的vector class內<br><img src="/../images/20220726_5.png"></li>
</ul>
<h2 id="Non-autoregressive-NAT"><a href="#Non-autoregressive-NAT" class="headerlink" title="Non-autoregressive(NAT)"></a>Non-autoregressive(NAT)</h2><p><img src="/../images/20220726_6.png" alt="upload successful"></p>
<ul>
<li>AT會把上一個字輸出出來，才產生下一個字元</li>
<li>NAT則是一次給好幾個START token，一次性產生整個sequence</li>
</ul>
<h3 id="Determine-NAT-output-length"><a href="#Determine-NAT-output-length" class="headerlink" title="Determine NAT output length"></a>Determine NAT output length</h3><ul>
<li>如何知道NAT decoder的長度哩?<ul>
<li>另外做一個pridictor去預測NAT該輸出的長度</li>
<li>假設一個句子的長度上限，看哪段輸出了END，就把字串砍到那個位置</li>
</ul>
</li>
</ul>
<h3 id="NAT優點"><a href="#NAT優點" class="headerlink" title="NAT優點"></a>NAT優點</h3><ul>
<li>相對於AT更平行化</li>
<li>可控的輸出長度(可以不用被動等到有END)<ul>
<li>可以發現AT是有點RNN、LSTM的思維</li>
<li>NAT是熱門的研究主題</li>
</ul>
</li>
<li>目前NAT的performance還是比較差<ul>
<li>原因:<a target="_blank" rel="noopener" href="https://youtu.be/jvyKmU4OM3c">multi-modality</a></li>
</ul>
</li>
</ul>
<h1 id="Transformer：Encoder-Decoder"><a href="#Transformer：Encoder-Decoder" class="headerlink" title="Transformer：Encoder-Decoder"></a>Transformer：Encoder-Decoder</h1><ul>
<li>現在焦點放到剛剛圖中圈起來的地方，這裡是encoder與decoder交會的地方(cross attention)</li>
</ul>
<p>圖示如下:<br><img src="/../images/20220726_7.png"></p>
<ul>
<li><p>做法很像一層self-attention在做的事情，只是q來自於decoder，而k,v來自於encoder</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/7472621">cross attention的相關論文</a></p>
<ul>
<li>這個方法不是來自transformer<br>    - 是先有cross才有self</li>
</ul>
</li>
<li><p>在原始論文內，decoder每一層cross attention都會拿encoder最後一層的輸出當輸入</p>
<ul>
<li>當然也有其他種連接方式，<a target="_blank" rel="noopener" href="https://arxov.org/abs/2005.08081">參考</a></li>
</ul>
</li>
</ul>
<hr>
<p> 以上是模型訓練好以後，模型怎麼去跑，接下來來談seq2seq類模型在training中碰到的小問題</p>
<h1 id="Seq2Seq：Training"><a href="#Seq2Seq：Training" class="headerlink" title="Seq2Seq：Training"></a>Seq2Seq：Training</h1><h2 id="Teacher-forcing"><a href="#Teacher-forcing" class="headerlink" title="Teacher forcing"></a>Teacher forcing</h2><p>下圖為Transformer的最終訓練展示圖<br><img src="/../images/pasted-31.png"></p>
<ul>
<li>每一次預測一個單位字，就是做一次的分類問題</li>
<li>每一個字視為一個分類問題，所以要minimize 這些所有預測+BEGIN、END各自的cross entropy</li>
<li>在訓練的時候會給decoder看ground truth<ul>
<li>這樣做法叫做Teacher forcing</li>
</ul>
</li>
</ul>
<h2 id="Copy-mechanism"><a href="#Copy-mechanism" class="headerlink" title="Copy mechanism"></a>Copy mechanism</h2><ul>
<li>很多任務不一定要decoder產生東西，而是從輸入中複製東西出來<ul>
<li>Ex. User: 你好，我是庫洛洛<br>    	Machine: 庫洛洛你好，很高興認識你<br>    - 機器不需要知道庫洛洛是誰，只要判別出人名複製就好</li>
</ul>
</li>
<li>機器聽不懂的話，也可以直接copy user input來再次詢問user是甚麼意思</li>
<li>應用: 閱讀文章的摘要</li>
<li>Pointer Network、Copy network</li>
</ul>
<h2 id="Guided-Attention"><a href="#Guided-Attention" class="headerlink" title="Guided Attention"></a>Guided Attention</h2><ul>
<li>TTS as example(語音合成)</li>
<li>看到過短的句子可能沒念完整，但讓她念好幾次卻發音成功<ul>
<li>嘗試強迫機器把所有看到的東西都看一遍 -&gt; guided attention</li>
</ul>
</li>
<li>在輸出嚴格的任務中頗好用(語音合成、辨識)<br><img src="/../images/pasted-27.png"></li>
<li>如果機器的觀看順序顛三倒四，表示這種attention可能會出問題</li>
<li>Guided attention就是強迫機器的attention有規範(ex.由左向右)</li>
<li>相關參考：Monotonic attention、Location-aware attention</li>
</ul>
<h2 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h2><p><img src="/../images/pasted-30.png"></p>
<ul>
<li><p>假設這個decoder只可能產生兩個token，要輸出一段sequence，前面都是直接輸出該輪概率最高的token，稱為「Greedy decoding」</p>
</li>
<li><p>但有沒有可能前面選擇非最佳的token，反倒導致後續的decode命中率更集中呢</p>
</li>
<li><p>Beam search找一個非最佳也不用爆搜的作法</p>
</li>
<li><p>關於這個演算法，有兩方論戰：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.09751">The curious case of neural test degeneration</a></p>
<ul>
<li>Randomness is needed for decoder when generating seq in some tasks (ex.TTS)</li>
</ul>
</li>
<li><p>老師認為在針對答案較為單一的任務，Beam search會表現比較好；而如果需要機器的想像力(空間大)的任務，則建議需要一些隨機性(故意加一些noise)</p>
</li>
<li><p>TTS任務中，需要在測試集加入雜訊</p>
<ul>
<li>true beauty lies in the crackks of imperfection</li>
</ul>
</li>
</ul>
<h2 id="Optimizing-evalyation-metrics"><a href="#Optimizing-evalyation-metrics" class="headerlink" title="Optimizing evalyation metrics?"></a>Optimizing evalyation metrics?</h2><ul>
<li>作業用BLEU Score作為判斷依據<ul>
<li>Decoder產生輸出以後，跟正確的句子做比較</li>
</ul>
</li>
<li>但訓練的時候資料分開計算，只能用cross entropy。這兩者之間未必正相關</li>
<li>不一定要挑decoder cross entropy效果最好的那個模型<ul>
<li>可否在training用BLEU score?<br>    	- 不容易，BLEU本身很複雜，不易微分<br>        - 當遇到opti無法解決的問題，就用RL(強化學習)硬train一發<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.06732">REF</a> (HARD)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Exposure-bias"><a href="#Exposure-bias" class="headerlink" title="Exposure bias"></a>Exposure bias</h2><p><img src="/../images/20220727_2.png"></p>
<ul>
<li><p>因為decoder在train的時候，前面的vector是看著ground truth在做，所以不會有「一步錯，步步錯的問題」</p>
</li>
<li><p>但當然在測試時不可能這麼做</p>
</li>
<li><p>可以故意在training裡面加入錯的ground truth來讓模型習慣前面有錯誤輸出的應對方式 -&gt; <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.03099">Scheduled Sampling</a></p>
</li>
<li><p>在transformer中會有所變化，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.07651">參考</a></p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/23/ML-2021-5-2-Transformer%EF%BC%88%E4%B8%8A%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/23/ML-2021-5-2-Transformer%EF%BC%88%E4%B8%8A%EF%BC%89/" class="post-title-link" itemprop="url">ML_2021_5-2 Transformer（上）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-23 20:40:26" itemprop="dateCreated datePublished" datetime="2022-07-23T20:40:26+08:00">2022-07-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>Transformer與BERT之間關係密切</li>
<li>Transformer是一種seq2seq的model</li>
</ul>
<h1 id="seq2seq簡介"><a href="#seq2seq簡介" class="headerlink" title="seq2seq簡介"></a>seq2seq簡介</h1><ul>
<li>由模型來決定輸出要多長</li>
<li>語音辨識、機器翻譯、speech translation就是應用(Hw4做的是分類模型)</li>
<li>speech translation不一定是speech recognition + machine translation，因為很多語言其實並沒有文字，或不普及(ex.台語:母湯?不行?)</li>
<li>其實有機會直接輸入台語的聲音資料，直接輸出中文(省略再翻譯的過程)</li>
</ul>
<h1 id="seq2seq應用"><a href="#seq2seq應用" class="headerlink" title="seq2seq應用"></a>seq2seq應用</h1><h2 id="台語範例"><a href="#台語範例" class="headerlink" title="台語範例"></a>台語範例</h2><ul>
<li>直接台語轉中文不是沒有可能</li>
<li>但是對於倒裝(文法)上的不同會有問題</li>
</ul>
<h2 id="Text-to-speech-TTS-synthesis"><a href="#Text-to-speech-TTS-synthesis" class="headerlink" title="Text-to-speech(TTS) synthesis"></a>Text-to-speech(TTS) synthesis</h2><ul>
<li>相反的，也有可能輸入中文然後合成出台語的聲音訊號作為輸出(範例是分為兩步驟，先把中文翻譯為台語羅馬拼音，之後再把他轉成聲音)<br><img src="/../images/20220724_7.png"></li>
</ul>
<h2 id="seq2seq-for-chatbot"><a href="#seq2seq-for-chatbot" class="headerlink" title="seq2seq for chatbot"></a>seq2seq for chatbot</h2><ul>
<li>Seq2seq也可以用在聊天機器人，輸入是一段文字，輸出則是response<ul>
<li>學習大量的日常對話(來自影集、連續劇等)</li>
</ul>
</li>
</ul>
<h2 id="seq2seq-in-NLP"><a href="#seq2seq-in-NLP" class="headerlink" title="seq2seq in NLP"></a>seq2seq in NLP</h2><ul>
<li>大多數的NLP應用，都可以想成是QA問題，而QA的問題，又可以透過seq2seq model來解決</li>
<li>不過對於NLP的任務，通常還是會針對任務特性做一個客製化的模型，seq2seq就像是瑞士刀一樣，對大多問題都可用，但不是最佳模型<ul>
<li>相關模型有另外一個課程: <a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/dlhlp/2020-spring.html"><br>DEEP LEARNING FOR HUMAN LANGUAGE PROCESSING 2020 SPRING</a></li>
</ul>
</li>
</ul>
<h2 id="seq2seq-in-others"><a href="#seq2seq-in-others" class="headerlink" title="seq2seq in others"></a>seq2seq in others</h2><ul>
<li>seq2seq也可以在一些輸出看似不像是seq的問題套用</li>
</ul>
<h3 id="syntactic-parsing"><a href="#syntactic-parsing" class="headerlink" title="syntactic parsing"></a>syntactic parsing</h3><pre><code>- ex. 文法解析(syntactic parsing)  
</code></pre>
<p><img src="/../images/pasted-25.png"><br>    - 把一個樹狀的結構用括號硬解成一個sequence，<a target="_blank" rel="noopener" href="https://arxiv.org/ab/1412.7449">參考</a></p>
<h3 id="multi-label-classification"><a href="#multi-label-classification" class="headerlink" title="multi-label classification"></a>multi-label classification</h3><ul>
<li>不同於multi-class，multi-label可以屬於多個class(同時屬於好幾類)</li>
<li>每個data對應的label個數可能不同，不能直接用分類模型輸出前n名</li>
<li>硬做seq2seq，輸出sequence就是class</li>
</ul>
<h3 id="object-detection"><a href="#object-detection" class="headerlink" title="object detection"></a>object detection</h3><pre><code>- 物件偵測也可以用seq2seq硬做  
</code></pre>
<p><img src="/../images/20220724_9.png"><br>    - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.12872">參考連結:End-to-End Object Detection with Transformers</a>
    </p>
<h1 id="Seq2seq結構"><a href="#Seq2seq結構" class="headerlink" title="Seq2seq結構"></a>Seq2seq結構</h1><ul>
<li>避免見樹不見林，先看seq2seq的完整模型架構</li>
<li>主要分為encoder跟decoder兩大區塊</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.3215">起源來自: Sequence to Sequence Learning with Neural Networks(2014)</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">現今形狀(transformer): Attention Is All You Need(2017)</a> (下圖)<br><img src="/../images/20220724_10.png"></li>
<li>Add &amp; Norm &#x3D; residual +layer norm</li>
<li>圈起來的地方是一個block，此圖範例只有1 block</li>
</ul>
<h2 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h2><ul>
<li>給定一排向量(input)，輸出一樣長度的向量</li>
<li>transformer中block用的技巧就是self-attention，好幾個block就是作好幾次self-attention<br><img src="/../images/20220724_11.png" alt="upload successful"></li>
<li>self-attention詳細的執行過程請參考CH4<ul>
<li>避免見樹不見林，這裡解釋一下encoder步驟，下方再分開講解每個區塊在幹嘛。encoder就是先把輸入轉成vectors(input embedding)，做完positional encoding以後，連續做好幾個block</li>
<li>每一個block做:self-attention -&gt; residual connection -&gt; layer norm -&gt; FC</li>
</ul>
</li>
</ul>
<h3 id="Residual-connection"><a href="#Residual-connection" class="headerlink" title="Residual connection"></a>Residual connection</h3><ul>
<li>原因暫且不討論，不過這種架構在DL被廣泛應用</li>
<li>做完self-attention之後在輸入到下一個block之前，需要進行一次的residual connection，就是把self-attention的輸出再加上自己原本的輸入<br><img src="/../images/20220724_12.png"></li>
</ul>
<h3 id="Layer-norm"><a href="#Layer-norm" class="headerlink" title="Layer norm"></a>Layer norm</h3><ul>
<li>原始transformer做完residual以後做<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">layer norm</a>，把單一feature的每個dimantion計算標準化</li>
</ul>
<blockquote>
<p>❗️ 與Batch norm的差別：<br>batch norm是把batch內不同筆data的同一個dimantion做標準化(橫向)；<br>而layer norm則是把同一筆data內不同dimantion做標準化(豎向)</p>
</blockquote>
<h3 id="Fully-connected-layer"><a href="#Fully-connected-layer" class="headerlink" title="Fully connected layer"></a>Fully connected layer</h3><ul>
<li>上述做完以後，輸出丟入FC訓練，然後『再做一次residual network以及norm』以後，輸出的結果，是一個block的計算</li>
</ul>
<h2 id="其他形狀的encoder"><a href="#其他形狀的encoder" class="headerlink" title="其他形狀的encoder"></a>其他形狀的encoder</h2><ul>
<li><p>BERT其實就是transformer的encoder</p>
</li>
<li><p>encoder的network架構是按照原始論文設計</p>
<ul>
<li>還有其他encoder設計<br>    - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.04745">參考1</a>、<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.07845">為何Batch norm不如layer norm?</a></li>
</ul>
</li>
<li><p>下章節介紹decoder</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/23/ML-LEE-2022-hw4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/23/ML-LEE-2022-hw4/" class="post-title-link" itemprop="url">ML_LEE_2022_hw4</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-23 01:12:30" itemprop="dateCreated datePublished" datetime="2022-07-23T01:12:30+08:00">2022-07-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/ml2022spring-hw4">作業題目</a></p>
<h1 id="可進行的目標"><a href="#可進行的目標" class="headerlink" title="可進行的目標"></a>可進行的目標</h1><p><img src="/../images/20220723_2.png"></p>
<ul>
<li>Medium : 0.70375 (train 1~1.5 hr)</li>
<li>Strong : 0.77750 (train 3~4 hr)</li>
<li>Boss : 0.86500 (train 2~2.5 hr )</li>
</ul>
<h2 id="調整transformer參數與pred-layer模型"><a href="#調整transformer參數與pred-layer模型" class="headerlink" title="調整transformer參數與pred_layer模型"></a>調整transformer參數與pred_layer模型</h2><ul>
<li>包括降低multi-head數量、feedforward dropout rate等等</li>
<li>降低pred-layer的linear connect layer層數 （這個問題複雜度不高，線性聯階層不用到兩層）</li>
</ul>
<h2 id="實作conformer"><a href="#實作conformer" class="headerlink" title="實作conformer"></a>實作conformer</h2><ul>
<li>把原本的 <code>self.encoder_layer</code>改掉，套用conformer模型</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.08100">conformer來源</a></li>
<li>記得修改Classifier的forward</li>
</ul>
<h2 id="更動transformer-x2F-comformer-encode層數"><a href="#更動transformer-x2F-comformer-encode層數" class="headerlink" title="更動transformer&#x2F;comformer encode層數"></a>更動transformer&#x2F;comformer encode層數</h2><ul>
<li>原本只有一層(就是<code>self.encoder_layer</code>)，可以改成呼叫<code>self.encoder</code>並修改Classifier的forward</li>
</ul>
<hr>
<h1 id="寫作業歷程"><a href="#寫作業歷程" class="headerlink" title="寫作業歷程"></a>寫作業歷程</h1><h2 id="2022-x2F-7-x2F-22"><a href="#2022-x2F-7-x2F-22" class="headerlink" title="2022&#x2F;7&#x2F;22"></a>2022&#x2F;7&#x2F;22</h2><p>這次作業transformer（self-attention）在NLP與語音辨識領域運用廣泛，是我想要好好學好的章節，不過這次作業給我挫敗感很大…<br>這份作業寫一整天，白天的時候在查<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.08100">comformer的paper</a>，但是因為對於transformer的觀念也不甚熟悉，所以照著李宏毅老師給的reference查到了<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/263031249">這篇paper（中文心得版）</a>，在大略的瞭解了transformer家族以後，最後回去看了一下<a target="_blank" rel="noopener" href="https://medium.com/ching-i/transformer-attention-is-all-you-need-c7967f38af14">別人看「Attention is all you need」的心得</a>，對照上課的筆記這才更有了概念</p>
<p>到了晚上終於要開始著手改code，首先在<a target="_blank" rel="noopener" href="https://pytorch.org/audio/stable/_modules/torchaudio/models/conformer.html#Conformer">torchaudio document</a>查到了他支援的conformer model API，開始套用以後才開始發現他的問題：看似簡單的forward修改其實困難重重！</p>
<p>首先是最基礎的問題，因為是pytorch菜雞，我甚至連forward的意涵是什麼都不知道，後來查到原來他是__call__會呼叫的函數，通常進行一個step（單位為一batch）的計算，而這份code在訓練時會呼叫model_fn</p>
<p>model_fn用意在於<br>    - 先做mels與labels的分類<br>    - 把資料放到GPU上<br>    - 呼叫<code>model.forward()</code>以及計算loss<br>    - 最後進行predict以後算出該step之acc</p>
<p>搞懂了forward以後，開始著手修改forward內部的code使他貼合conformer，但因為這樣所以我也必須去查torchaudio.models.Conformer這個class裡面的forward會怎麼運作（巢狀模型），查到的資料如下：<br><img src="/../images/20220722_1.png"><br>註解寫到lengths with shape(B,)，因此我很直觀的直接打了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># out: (batch size, length, d_model)</span><br><span class="line">self.encoder(out,(out[0],) ) </span><br></pre></td></tr></table></figure>
<p>這麼做跳了一個錯誤：他抓不到<code>out[0]</code>的shape，我這才發現他要的是torch.Tensor type，於是我改成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.encoder(out, torch.tensor(out[0],) )</span><br></pre></td></tr></table></figure>
<p>仔細研究torch.tensor用法以後終於可以正確地塞tensor進去，但是我卻鬼打牆的一直卡在lengths這個參數的shape不對，從錯誤碼發現跟ket_padding_mask有關，幾經波折以後看到<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/353365423">這篇</a>才發現他要吃的參數是(batch size,sequence length)，最後修正成了這樣</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out = self.encoder(out,torch.tensor([out[0],out[1]]))</span><br></pre></td></tr></table></figure>
<p>卻碰到了新的錯誤碼：<br><img src="/../images/20220722_2.png"><br>留待明天處理…</p>
<hr>
<h2 id="2022-x2F-7-x2F-23"><a href="#2022-x2F-7-x2F-23" class="headerlink" title="2022&#x2F;7&#x2F;23"></a>2022&#x2F;7&#x2F;23</h2><p>果然是昨天腦袋昏到不好思考，把out.size(0)跟out[0].size()搞混，雖說後來修正了，但還是套不進去。對於tensor dim的掌握力太差了…<br><img src="/../images/20220723_1.png"></p>
<p>後來改用<a target="_blank" rel="noopener" href="https://github.com/lucidrains/conformer">這個conformer</a>來實作，多了很多我不知道幹嘛用的參數，而且看似不能增加layer數orz…，先求能跑吧，感覺達不到strong base line了，參數如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">self.encoder_layer = ConformerBlock(</span><br><span class="line">    dim = d_model,</span><br><span class="line">    dim_head = 64,</span><br><span class="line">    heads = 2,</span><br><span class="line">    ff_mult = 4,</span><br><span class="line">    conv_expansion_factor = 2,</span><br><span class="line">    conv_kernel_size = 31,</span><br><span class="line">    attn_dropout = 0.,</span><br><span class="line">    ff_dropout = 0.,</span><br><span class="line">    conv_dropout = 0.</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>要注意的是他的forward吃的參數是<code>(length, batch size, d_model)</code>，所以permute那行註解要拿掉，這方面沒有統一真的很煩人，最後Classifier forward code如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, mels):</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	args:</span><br><span class="line">		mels: (batch size, length, 40)</span><br><span class="line">	return:</span><br><span class="line">		out: (batch size, n_spks)</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	# out: (batch size, length, d_model)</span><br><span class="line">	out = self.prenet(mels)</span><br><span class="line">	# out: (length, batch size, d_model)</span><br><span class="line">	out = out.permute(1, 0, 2)</span><br><span class="line">	# The encoder layer expect features in the shape of (length, batch size, d_model).</span><br><span class="line"> #out = self.encoder_layer(out)</span><br><span class="line">  # Do 2 conformer layer</span><br><span class="line">#	out = self.encoder_layer(out)</span><br><span class="line">	out = self.encoder_layer(out)</span><br><span class="line">	# out: (batch size, length, d_model)</span><br><span class="line">	out = out.transpose(0, 1)</span><br><span class="line">	# mean pooling</span><br><span class="line">	stats = out.mean(dim=1)</span><br><span class="line"></span><br><span class="line">	# out: (batch, n_spks)</span><br><span class="line">	out = self.pred_layer(stats)</span><br><span class="line">	return out</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>中間有個<code>out = self.encoder_layer(out)</code>被註解了，我原本是想說既然參數不能設定layer數，那就讓他跑兩次也可以是兩輪，不過我想太美了，因為GPU多線程的關係，讓整個tqdm的log大暴走，正確性也打個問號，搞不好會碰到race condition…，因為這是這個模型設計者沒設計到的部分(或是我菜到沒發現QQ)，所以最後決定讓他就單層去跑</p>
<hr>
<h1 id="作業結果"><a href="#作業結果" class="headerlink" title="作業結果"></a>作業結果</h1><p>Train了70000 step (大約1 hr)，跟strong差一點點，但acc還在上升，感覺是練不夠久<br><img src="/../images/20220723_4.png"><br>補了以下的code讓他繼續撿剛剛的模型train</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if exists(&quot;./model.ckpt&quot;):</span><br><span class="line">	model.load_state_dict(torch.load(&quot;./model.ckpt&quot;))</span><br><span class="line">	print(&quot;Exist model detected, continue training...&quot;)</span><br><span class="line">       if model.training() == False:</span><br><span class="line">		model.train()</span><br></pre></td></tr></table></figure>
<p>得到命中率&#x3D;79%<br><img src="/../images/20220723_7.png"></p>
<p>但提交的時候卻出了大問題，測試的結果非常差<br><img src="/../images/20220723_5.png"><br>有查到一篇心得(下圖)也是成績暴跌，推測測試集跟訓練集的分布差很多(不過那篇作者是碰到切割長度的問題，這我沒有改不該有影響才對)，這份作業做到這吧，沒力氣修正了orz，再修也會淪為調參之流，感覺學不到甚麼。<br>結論姑且下在data mismatch，不過模型參數與notebook我會保留在github，將來靈光乍現的時候再來看看</p>
<p><img src="/../images/20220723_6.png"></p>
<h1 id="Report"><a href="#Report" class="headerlink" title="Report"></a>Report</h1><p><img src="/../images/pasted-23.png"></p>
<ol>
<li><p>因為卡住所以我有多看幾個varient</p>
<ul>
<li>linformer : 藉由attention matrix在做sorftmax轉化以後非行滿秩的特性，降低attention matrix的維度，使得transformer的複雜度可以降到線姓<br>    - Sparse Attention (FP&#x2F;LP) : 因為傳統transformer的複雜度過高，所以藉由讓self-attention關注的範圍由全局變成局部性，降低計算時間。FP(fixed patterns)是固定self-attention窗口大小，LP則是把它的大小變成一個可學習的參數</li>
</ul>
</li>
<li><p>因為transformer雖然可以考慮sequence的全局性，但對於自己的重視程度較為有限，而CNN則可以對局部性的特徵著重關注，兩者混合起來可以達到互補提高準確性的效果。</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/22/Efficient-Transformers-A-Survey%E7%AD%86%E8%A8%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/22/Efficient-Transformers-A-Survey%E7%AD%86%E8%A8%98/" class="post-title-link" itemprop="url">Efficient Transformers:A Survey筆記</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-22 14:53:52" itemprop="dateCreated datePublished" datetime="2022-07-22T14:53:52+08:00">2022-07-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.06732">論文原文連結</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/263031249">翻譯版連結</a></li>
</ul>
<p>本篇論文主要介紹Transformer的各種變體，以及他的優缺點</p>
<h1 id="What-is-Transformer"><a href="#What-is-Transformer" class="headerlink" title="What is Transformer?"></a>What is Transformer?</h1><ul>
<li>Transformer採用了Self-attention技術，並屏棄了以往對於序列資料所採用的RNN</li>
<li>序列資料得以考慮全局性，不再像RNN那樣是序列依賴性</li>
<li>Transformer可以看作是CNN，只是每一個vector考慮的依賴性是整張圖片（把一個sequence鋪成平面，每一個vector視為一個pixel，vector維數&#x3D;channel）</li>
</ul>
<h2 id="Transformer的問題"><a href="#Transformer的問題" class="headerlink" title="Transformer的問題"></a>Transformer的問題</h2><ul>
<li>目前的計算複雜度過高，是$O(n^2)$，使得transformer難以在輕模型下使用</li>
<li>目前為了解決transformer過度肥大的問題，出現了許多的變種</li>
</ul>
<h2 id="Dense-Transformer"><a href="#Dense-Transformer" class="headerlink" title="Dense Transformer"></a>Dense Transformer</h2><ul>
<li>transformer的起源</li>
<li>待補</li>
</ul>
<h2 id="Sparse-Attaention"><a href="#Sparse-Attaention" class="headerlink" title="Sparse Attaention"></a>Sparse Attaention</h2><ul>
<li>這裡開始是為了降低複雜度而延伸的變種</li>
<li>限制self-attention的參考範圍，使全面參考變成部分參考</li>
<li></li>
</ul>
<h3 id="Fixed-patterns-FP"><a href="#Fixed-patterns-FP" class="headerlink" title="Fixed patterns(FP)"></a>Fixed patterns(FP)</h3><ul>
<li>參考範圍作為hyper-parameter存在，是訓練前先決定好的固定值</li>
<li>常見方法有<ul>
<li>Blockwise pattern : 把輸入序列切成block(batch)，複雜度降為$O(B^2)$，變成$(\frac{N}{B}*B^2)$<ul>
<li>導致序列不連貫，效果”可能”有限</li>
</ul>
</li>
<li>Strided pattern : 變得真的非常像CNN，採用滑動kernel的的方式，每個vector只attention自己的那塊kernel<ul>
<li>建立在假設NLP在多數情況下都是具備局部相關的前提下，免去過度參考太遠的vector所浪費的計算</li>
</ul>
</li>
<li>Compressed pattern : 超級無敵像CNN(欸)，用把vector壓成平面的角度會更好思考，其實就是pooling，把幾個token池化，變成一個vector後才做attention，個人認為複雜度會降為$O(\frac{N^2}{kernel_size})$</li>
</ul>
</li>
</ul>
<h3 id="Learnable-patterns-LP"><a href="#Learnable-patterns-LP" class="headerlink" title="Learnable patterns(LP)"></a>Learnable patterns(LP)</h3><ul>
<li>相對於ＦＰ，ＬＰ是把參考範圍也作為weight給機器去學習</li>
<li>比如routing transformer是對token向量進行k-means來將整體序列切成多個子序列</li>
</ul>
<h3 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h3><p>這裡看不太懂，先貼原文</p>
<p>memory乍一听好像有点让人摸不着头脑，其实想法也很简单。最开始是在19年的set transformer上使用。一般来说做multihead self-attention时，Q&#x3D;K&#x3D;V&#x3D;X（X为输入序列，长度为n），而在set transformer中，作者先单独设置了m个向量（m是超参数），然后这m个向量与X做multihead attention，得到m个临时向量（这些个临时变量我们就称作“temporary memory”），接着把X与这m个临时向量再做一次multihead attention得到输出。这个过程其实就是利用这m个向量将输入序列X的信息先通过attention进行压缩，再通过attention还原，达到抽取输入序列的特征的目的。但是在压缩编码解码的过程中肯定会有信息损失，所以后来的改进方法是引入全局记忆，即设置一些token，它们可以与所有的token进行注意力交互，比如bert的[CLS]，由于这些token的数目远小于序列长度，因此也不会给计算带来负担，而且往往携带了整个输入序列的信息，这就是我们可以在bert上用[CLS]作文本分类任务的原因。</p>
<h3 id="Low-rank-methods"><a href="#Low-rank-methods" class="headerlink" title="Low rank methods"></a>Low rank methods</h3><ul>
<li>Attention matrix在經過softmax轉化以後會是不滿秩的(存在linear dependent,rank !&#x3D; N)，所以我們可以把矩陣降維</li>
<li>k,v向量可以因此映射到kxd維，因為k為hyper-parameter，複雜度可降至 $O(nk)$</li>
<li>範例：linformer</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.04768">相關論文</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/13/ML-2021-4-2-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%EF%BC%88%E4%B8%8B%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/13/ML-2021-4-2-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%EF%BC%88%E4%B8%8B%EF%BC%89/" class="post-title-link" itemprop="url">ML_2021_4-2 自注意力機制（下）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-13 16:30:46" itemprop="dateCreated datePublished" datetime="2022-07-13T16:30:46+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>延續上篇的內容，b其實可以水平同時計算</li>
</ul>
<h2 id="線性代數的角度理解"><a href="#線性代數的角度理解" class="headerlink" title="線性代數的角度理解"></a>線性代數的角度理解</h2><p><img src="/../images/pasted-18.png"></p>
<p><img src="/../images/pasted-19.png"><br>把它延展到$\alpha_{4,4}$</p>
<p><img src="/../images/20220713_7.png"></p>
<ul>
<li>之後再對A做softmax(或其他激發函數)得到A’</li>
</ul>
<p><img src="/../images/20220713_8.png"></p>
<h3 id="總圖"><a href="#總圖" class="headerlink" title="總圖"></a>總圖</h3><p><img src="/../images/20220713_9.png"></p>
<ul>
<li>上圖中只有$W^{q,k,v}$training data訓練</li>
</ul>
<h2 id="Multi-head-self-attention"><a href="#Multi-head-self-attention" class="headerlink" title="Multi-head self-attention"></a>Multi-head self-attention</h2><ul>
<li>可以把q,k,v產生多個<br><img src="/../images/pasted-20.png"></li>
</ul>
<h2 id="Positional-encoding"><a href="#Positional-encoding" class="headerlink" title="Positional encoding"></a>Positional encoding</h2><ul>
<li>每一個vector出現在sequence的不同位置可能會有不同的意義</li>
<li>前述之計算並無考慮到相對位置，大家算法都一樣，也都有全部平等的計算</li>
<li>為每一個位置設置一個vector $e^i$，再把這個vector加上$a^i$</li>
<li>$e^i$可以透過某個function產生，也可能是hand-crafted</li>
<li>或也可以是learn by data</li>
<li>仍然是尚待研究的主題</li>
</ul>
<h2 id="Self-attention應用"><a href="#Self-attention應用" class="headerlink" title="Self-attention應用"></a>Self-attention應用</h2><ul>
<li>Bert（NLP）</li>
<li>transformer</li>
</ul>
<h2 id="Truncated-self-attention"><a href="#Truncated-self-attention" class="headerlink" title="Truncated self-attention"></a>Truncated self-attention</h2><ul>
<li>當sequence很大，我們的attention matrix會非常大</li>
<li>一次訓練的時候不要看所有sequence，看某一段就好</li>
</ul>
<h2 id="Self-attention-for-image"><a href="#Self-attention-for-image" class="headerlink" title="Self-attention for image"></a>Self-attention for image</h2><ul>
<li>相片也可以看作是一個vector set，ex. 給定一個5x10 pixel的彩色圖片<ul>
<li>把channel當作一個vector(RGB)</li>
<li>則一張圖片是一個5x10的vector set</li>
</ul>
</li>
<li>Self-attention GAN</li>
<li>DETR</li>
</ul>
<h3 id="Self-attention-v-s-CNN"><a href="#Self-attention-v-s-CNN" class="headerlink" title="Self-attention v.s CNN"></a>Self-attention v.s CNN</h3><ul>
<li>CNN會考慮receptive field</li>
<li>self-attention則是會考慮整張圖（整個vector set的vector）<br><img src="/../images/20220713_10.png"></li>
<li>self-attention可以說是複雜版、更自由的ＣＮＮ</li>
<li>receptive field變成可學習、控制的大小(truncated self attention)<ul>
<li>Ref. CNN章節<br><img src="/../images/20220713_11.png"></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.93584">這篇論文</a>用嚴謹的數學證明CNN $\subset$ self-attention</li>
<li>Ref.一開始的章節，我們知道更flexible的模型需要更多training data<br><img src="/../images/20220713_12.png"></li>
</ul>
<h3 id="Self-attention-v-s-RNN"><a href="#Self-attention-v-s-RNN" class="headerlink" title="Self-attention v.s RNN"></a>Self-attention v.s RNN</h3><ul>
<li>RNN不再講到，因為self-attention大多可取代</li>
<li>RNN也是處理input是sequence的情況<ul>
<li>第一個vector做RNN，輸出丟FC，產生輸出，也加入第二個vector丟入RNN…<br><img src="/../images/pasted-22.png"></li>
</ul>
</li>
<li>RNN不可以平行處理，self-attention可以</li>
<li>RNN難以考慮全面（左右更好），self-attention則可以（天涯若壁鄰）</li>
<li>Self-attention加上一些東西以後，其實也能變成RNN(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.16236">ref.</a>)</li>
</ul>
<h3 id="Self-attention-for-graph"><a href="#Self-attention-for-graph" class="headerlink" title="Self-attention for graph"></a>Self-attention for graph</h3><ul>
<li>因為edge，所以我們知道哪兩個節點之間會有關聯性</li>
<li>Attention matrix可以只計算有相連的部分就好<br><img src="/../images/20220713_14.png"></li>
<li>這算是一種Graph neural network (GNN)</li>
</ul>
<h4 id="GNN-REF"><a href="#GNN-REF" class="headerlink" title="GNN REF."></a>GNN REF.</h4><p><a target="_blank" rel="noopener" href="https://youtu.be/eybCCtNKwzA">影片1</a></p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8">影片2</a></p>
<h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><ul>
<li>Self-attention有很多種變形</li>
<li>他的缺點是計算量過大(廣義版CNN…汗)</li>
<li>self-attention最早用於transformer<ul>
<li>有時候叫transformer其實就是指self-attention</li>
</ul>
</li>
<li>performance跟speed的平衡</li>
<li>介紹各式各樣的transformers的變形：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A survey</a><br><img src="/../images/20220713_15.png"></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/13/ML-2021-4-1-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%EF%BC%88%E4%B8%8A%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/13/ML-2021-4-1-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%EF%BC%88%E4%B8%8A%EF%BC%89/" class="post-title-link" itemprop="url">ML_2021_4-1 自注意力機制（上）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-13 10:33:00" itemprop="dateCreated datePublished" datetime="2022-07-13T10:33:00+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>至今為止，我們network input 都是一個vector</li>
<li>那如果輸入是可變動的一排向量（sequence）呢？</li>
</ul>
<h2 id="Vector-set-as-input"><a href="#Vector-set-as-input" class="headerlink" title="Vector set as input"></a>Vector set as input</h2><ul>
<li>長度不一的句子就是一個範例</li>
<li>關於word embedding如何得到，可以參考<a target="_blank" rel="noopener" href="https://youtu.be/X7PH3NuYW0Q">這個影片</a></li>
<li>現在的文字基本上都是被word embedding過，而句子就是一連串了文字向量</li>
<li>一段聲音訊號也是一個範例（25 millisecond），一個向量稱為『frame』</li>
<li>一個graph也是一連串的向量<ul>
<li>Drug discovery中一個分子，可以看做一個graph</li>
<li>社群媒體中，人（節點）可以是一個向量，ex.性別、年齡、工作等等</li>
</ul>
</li>
</ul>
<h2 id="What-is-the-output"><a href="#What-is-the-output" class="headerlink" title="What is the output?"></a>What is the output?</h2><h3 id="Type-1-本課專注"><a href="#Type-1-本課專注" class="headerlink" title="Type 1 (本課專注)"></a>Type 1 (本課專注)</h3><ul>
<li>每一個vector都會有一個label</li>
<li>POS tagging（詞性標注），每一個詞彙都要對應一個詞性</li>
<li>語音，每一段frame都會有一個Pheonic</li>
<li>Social network，對每一個人可能會有一種廣告投放方式</li>
</ul>
<h3 id="Type-2-hw4"><a href="#Type-2-hw4" class="headerlink" title="Type 2 (hw4)"></a>Type 2 (hw4)</h3><ul>
<li>一整個sequence輸出一個label</li>
<li>Sentiment analysis: 機器去判讀一段句字是正面的還是負面</li>
<li>給定一段音訊，分辨它是哪個人說的</li>
</ul>
<h3 id="Type-3"><a href="#Type-3" class="headerlink" title="Type 3"></a>Type 3</h3><ul>
<li>不知道輸出幾個label</li>
<li>稱為sequence to sequence(seq2seq)</li>
</ul>
<h2 id="Sequence-labeling"><a href="#Sequence-labeling" class="headerlink" title="Sequence labeling"></a>Sequence labeling</h2><ul>
<li>對於每個向量，要做一個label</li>
</ul>
<h3 id="First-approach"><a href="#First-approach" class="headerlink" title="First approach"></a>First approach</h3><ul>
<li>直接用fully connect network</li>
<li>問題出現：同樣輸入就會有同樣輸出，但是不能保證兩個vector之間是否有關連</li>
<li>需要consider the context</li>
</ul>
<h3 id="Second-approach"><a href="#Second-approach" class="headerlink" title="Second approach"></a>Second approach</h3><ul>
<li>直接給fully connected network整個window (hw2就是這樣做的)<br><img src="/../images/20220713_1.png"></li>
<li>問題：如果今天的任務是得要考慮整個sequence怎辦<ul>
<li>sequence長度有長有短，window大小要變動，而且運算量非常大又導致overfitting</li>
</ul>
</li>
</ul>
<h3 id="Third-approach"><a href="#Third-approach" class="headerlink" title="Third approach"></a>Third approach</h3><ul>
<li>採用Self-attention技術，先把向量加工再個別丟入全連階層<br><img src="/../images/20220713_2.png" alt="upload successful"></li>
<li>黑框框向量表示考慮過前後文的加工向量</li>
<li>Self-attention可以有很多層</li>
<li>經典論文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention is all you need</a><ul>
<li>Transformer</li>
</ul>
</li>
<li>Self-attention內部結構如下：<br><img src="/../images/20220713_3.png"></li>
<li>找出relevant vectors in a sequence，關聯度以$\alpha$表示</li>
<li>計算$\alpha$比較常見的做法是做內積，兩個向量各自乘一個矩陣($W^q、W^k等$)以後再做內積<ul>
<li>之後課程先只用這個方法</li>
</ul>
</li>
</ul>
<h2 id="How-to-apply"><a href="#How-to-apply" class="headerlink" title="How to apply"></a>How to apply</h2><p><img src="/../images/pasted-17.png"></p>
<ul>
<li>$q^1$表示輸入向量$a^1$對$W^q$矩陣相乘的結果</li>
<li>$k^i$則表示內積的另一個算子，表示$a^i * W^k$以後的結果</li>
<li>softmax不一定是唯一解，只是常見（用他沒有理由）</li>
<li>得出$\alpha’$以後，繼續根據他抽取sequence中重要的資訊<br><img src="/../images/20220713_5.png"></li>
<li>最後再把$\alpha’$乘上$W^v$，一個向量得到的分數越高，則越可能會dominate抽取出的結果</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://medium.com/ching-i/transformer-attention-is-all-you-need-c7967f38af14">參考文章</a></p>
<ul>
<li>q:query，就是輸入的vector，用於與k做內積來判斷相似性</li>
<li>k:key，指序列中的所有詞向量</li>
<li>v:value，指實際的序列內容</li>
<li>q,k內積的過程稱為Dot-product Attention</li>
<li>兩個vector之間的關聯越大，則 $\alpha$ 越大</li>
<li>上面步驟講到的都是encoder</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/11/ML-LEE-2022-hw3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/11/ML-LEE-2022-hw3/" class="post-title-link" itemprop="url">ML_LEE_2022_hw3</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-11 21:24:56" itemprop="dateCreated datePublished" datetime="2022-07-11T21:24:56+08:00">2022-07-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/ml2022spring-hw3b">作業題目</a></p>
<h1 id="可進行的目標"><a href="#可進行的目標" class="headerlink" title="可進行的目標"></a>可進行的目標</h1><h2 id="Data-augmentation-training"><a href="#Data-augmentation-training" class="headerlink" title="Data augmentation (training)"></a>Data augmentation (training)</h2><ul>
<li><p>搜尋torchvision.transform，<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/transforms.html">docs</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://chih-sheng-huang821.medium.com/03-pytorch-dataaug-a712a7a7f55e">有用的文章：Pytorch提供之torchvision data augmentation技巧</a></p>
</li>
<li><p>transform.randomRotate   ,   resize等等</p>
</li>
<li><p>最後必須要toTensor，model只吃pytorch的tensor不吃PIL library的</p>
</li>
</ul>
<h2 id="Adv-Data-augmentation-mixed-up-training"><a href="#Adv-Data-augmentation-mixed-up-training" class="headerlink" title="Adv. Data augmentation - mixed up (training)"></a>Adv. Data augmentation - mixed up (training)</h2><ul>
<li><p>mixup (把兩個影像混再一起，變成多重label)</p>
</li>
<li><p>cross entropy loss function需要重寫</p>
</li>
</ul>
<h2 id="Module-selection-搜尋torchvision-models"><a href="#Module-selection-搜尋torchvision-models" class="headerlink" title="Module selection (搜尋torchvision.models)"></a>Module selection (搜尋torchvision.models)</h2><ul>
<li><p>AlexNet</p>
</li>
<li><p>VGG系列</p>
</li>
<li><p>ResNet</p>
</li>
<li><p>SqueezeNet</p>
</li>
</ul>
<h2 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross validation"></a>Cross validation</h2><ul>
<li><p>每次訓練更換dataset</p>
</li>
<li><p>ensemble</p>
</li>
</ul>
<p><img src="/../images/20220711_1.png" alt="upload successful"></p>
<hr>
<p>以下是我對這作業除了Base sample code以外所做的變化，針對上述目標，有做的我才會列出來</p>
<h1 id="First-Approach"><a href="#First-Approach" class="headerlink" title="First Approach"></a>First Approach</h1><p>Note: 因為這次作業放在kaggle上寫&amp;跑，而kaggle設計在結束比賽之前不能公開notebook，所以不能內嵌frame&#x3D; &#x3D;，下面只能先直接貼上code代替</p>
<!--<iframe src="https://codepen.io/gretema/embed/eYOjPJx?height=265&theme-id=default&default-tab=html,result" width="100%" height="300" frameborder="0" loading="lazy" allowfullscreen></iframe> -->
<h2 id="Data-argumentation"><a href="#Data-argumentation" class="headerlink" title="Data argumentation"></a>Data argumentation</h2><ul>
<li>改動trains transform，新增兩個處理，一個是讓照片有0.6的可能水平翻轉，另一個則是把它做normalization<ul>
<li>針對個別batch算出mean,std做normalization的方法我沒找到，目前直接套網路上常用的3 channel RGB圖之平均值當參數 (mean &#x3D; [0.5,0.5,0.5] std &#x3D; [0.1, 0.1, 0.1])<br>    - 這樣做觸犯1個問題，我的normalization是單一標準對所有data做，還是對每個batch單獨這樣做？在這個case沒差，不過這是因為數字寫死，normalization做的實在有夠醜&#x3D; &#x3D;<br>    - 觸犯另一個問題是，我的layer之間有沒有再進行一次normalization呢？有待釐清<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mean = [0.5, 0.5, 0.5]</span><br><span class="line">std = [0.1, 0.1, 0.1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_tfm = transforms.Compose([</span><br><span class="line">    # Resize the image into a fixed shape (height = width = 128)</span><br><span class="line">    transforms.Resize((128, 128)),</span><br><span class="line">    # You may add some transforms here.</span><br><span class="line">    transforms.RandomHorizontalFlip(p=0.6),</span><br><span class="line">    # ToTensor() should be the last one of the transforms.</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean,std),</span><br><span class="line">])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="Module-selection"><a href="#Module-selection" class="headerlink" title="Module selection"></a>Module selection</h2><ul>
<li>這裡我直接選用VGG11取代原本的自訂模型Classifier</li>
</ul>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><ul>
<li>跑了至少100 epoch，結果training acc達到0.97，validate acc卻只有0.1x而且不再有更好結果，明顯的overfitting &#x3D; &#x3D;</li>
<li>可能要考慮加入cross validation</li>
<li>可能是data augmentation太弱？</li>
<li>可能是VGG11模型太強(彈性過高)</li>
<li>我耍笨不小心把訓練的過程輸出洗掉了，沒有圖片QQ</li>
</ul>
<hr>
<h1 id="Second-Approach"><a href="#Second-Approach" class="headerlink" title="Second Approach"></a>Second Approach</h1><h2 id="Module-Selection"><a href="#Module-Selection" class="headerlink" title="Module Selection"></a>Module Selection</h2><ul>
<li>單純的把VGG11模型改回Classification</li>
</ul>
<h2 id="Result-1"><a href="#Result-1" class="headerlink" title="Result"></a>Result</h2><ul>
<li>看起來好像沒什麼變化，看來不是VGG模型導致overfitting</li>
<li>可能是normalization的部分出問題了，接下來嘗試把normalization改掉，用VGG train看看<ul>
<li>如果這樣成功的話，只能說應該是normalization把數字改成詭異的形狀了，算是一種人為mismatch吧<br><img src="/../images/20220711_2.png"></li>
</ul>
</li>
</ul>
<hr>
<h1 id="Third-Approach"><a href="#Third-Approach" class="headerlink" title="Third Approach"></a>Third Approach</h1><h2 id="Transform"><a href="#Transform" class="headerlink" title="Transform"></a>Transform</h2><ul>
<li>移除normalization</li>
</ul>
<h2 id="Result-2"><a href="#Result-2" class="headerlink" title="Result"></a>Result</h2><ul>
<li><p>acc爆增回正常範圍了，看來真的是normalization的鍋qwq<br><img src="/../images/20220711_3.png"></p>
</li>
<li><p>上面這是最高紀錄，valid acc 有70%，位於epoch 32，我後來一直跑到epoch 52都沒看到更好的分數，就先卡掉了(看起來進medium概率近乎於零&#x3D; &#x3D;)<br><img src="/../images/20220712_4.png"></p>
</li>
<li><p>不過雖然70%比起之前的acc是大躍進，距離medium仍有一段距離，training acc也到達瓶頸，看起來是需要提高data augmentation的時候了</p>
</li>
</ul>
<h1 id="Forth-Approach"><a href="#Forth-Approach" class="headerlink" title="Forth Approach"></a>Forth Approach</h1><ul>
<li>偷偷參考了一下<a target="_blank" rel="noopener" href="https://github.com/Joshuaoneheart/ML2022_all_A_plus/blob/main/hw3.md">學長</a>的筆記，發現新招數「AutoAugmentation」，適用在Transform內</li>
<li>這招的原理來自於<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.09501.pdf">這篇論文</a>，可以從data裡面學到如何排Transform Augment</li>
<li>同時也從學長的筆記學到，因為code裡面有用BatchNorm2d (batch Normalization)，所以batchSize大一點會比較有利<ul>
<li>這好像就是元兇嗎orz，多做一次norm<br>    - 不過我只有在助教寫的Classification有看到norm，因為用的是VGG11，所以不確定是否仍然有用norm(沒看源碼XD)</li>
</ul>
</li>
</ul>
<h2 id="Transform-1"><a href="#Transform-1" class="headerlink" title="Transform"></a>Transform</h2><ul>
<li>新增<code>transforms.AutoAugment()</code></li>
<li>新增<code>transforms.RandomRotation(degrees = 32)  //rotate+-32度</code></li>
</ul>
<h2 id="BatchSize"><a href="#BatchSize" class="headerlink" title="BatchSize"></a>BatchSize</h2><ul>
<li>修改為96</li>
</ul>
<h2 id="Result-3"><a href="#Result-3" class="headerlink" title="Result"></a>Result</h2><ul>
<li>這次讓他跑了一整晚，充分認識到kaggel save&amp;run的重要性&#x3D; &#x3D;</li>
<li>直接掛網頁按run all如果網頁停止回應或是閒置過久就沒了</li>
<li>這次訓練時間大約10hr，仍然沒有得到最終結果-&gt;網頁爆了<br><img src="/../images/20220712_1.png"></li>
<li>練到了291 epoch，感覺valid的acc就上不去了… 只能止步medium嗎</li>
</ul>
<hr>
<h1 id="Fifth-Approach"><a href="#Fifth-Approach" class="headerlink" title="Fifth Approach"></a>Fifth Approach</h1><ul>
<li>最後又重跑了一次training &#x3D; &#x3D;，沒有作任何更動，GPU quota還有18 hr 希望夠用…</li>
</ul>
<h2 id="Result-Finale"><a href="#Result-Finale" class="headerlink" title="Result (Finale)"></a>Result (Finale)</h2><p><img src="/../images/20220712_2.png"><br>&#x3D; &#x3D;凸</p>
<ul>
<li>最終在epoch 389的時候終止了，最好的epoch在252，其實已經很接近當初的150 patience了</li>
<li>好在best model parameters有保存下來，接下來就是load model之後直接predict了</li>
</ul>
<h2 id="最終成果"><a href="#最終成果" class="headerlink" title="最終成果"></a>最終成果</h2><p><img src="/../images/20220712_3.png"><br><a target="_blank" rel="noopener" href="https://www.kaggle.com/code/laxiflora/ml2022hw3-sample-code-training-predictin">Notebook連結</a></p>
<h1 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h1><p>這次作業是CNN的範例題，這次的圖像辨識題真的讓人思考到了如何去優化他，助教提供的sample code省去了start from scratch的痛苦，讓我們能專注在實作理論的部分</p>
<p>其實很多的技術(Batch normalization、crossEntropy+softmax、data Batch等等)都已經被函數包進去一次做好了，正常的時候是不會發現到他們的存在，這或許也間接印證了他們是十分有效提高命中率的方法吧。而真的需要實作的part其實不多，比較難的是要去翻出他們的document一一認識他們的結構並理解功能，這才是最難的部分</p>
<p>助教的sample code寫得蠻精美的，甚至有看出在自訂結構其實有保留空間讓學生自行切分training set跟valid set(可能是用於做cross validation用的)，考量到讓學生改進而保留空間，真的厲害！除了實作理論與閱讀結構以外，最重要的估計就是看懂這個訓練過程的資料結構了吧~</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/09/ML-2021-3-1-%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/09/ML-2021-3-1-%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/" class="post-title-link" itemprop="url">ML_2021_3-1 卷積神經網路</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-09 16:13:09" itemprop="dateCreated datePublished" datetime="2022-07-09T16:13:09+08:00">2022-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>network架構的其中一種變形:CNN</li>
<li>此講專注在CNN專門用在影像上的講解(目前也普遍用在影像)</li>
</ul>
<h1 id="Image-classification-Version-1"><a href="#Image-classification-Version-1" class="headerlink" title="Image classification - Version 1"></a>Image classification - Version 1</h1><ul>
<li>我們需要假設圖片輸入的大小都是固定的<ul>
<li>如果大小不一，就得要rescale</li>
</ul>
</li>
<li>是classification問題，所以輸出one-hot vector (向量的長度代表你能分出多少種類別)</li>
</ul>
<h2 id="如何把影像當作輸入"><a href="#如何把影像當作輸入" class="headerlink" title="如何把影像當作輸入"></a>如何把影像當作輸入</h2><ul>
<li>一張圖片其實是一個3維的Tensor (RGB)<ul>
<li>Tensor(長跟寬)，並有3個Channel</li>
</ul>
</li>
<li>我們把3維的Tensor拉直，成為一個向量 (這也是為何影像大小需要相同)</li>
</ul>
<h2 id="Train-with-fully-connected-network？"><a href="#Train-with-fully-connected-network？" class="headerlink" title="Train with fully connected network？"></a>Train with fully connected network？</h2><ul>
<li>如果我們依然使用fully connected network來訓練，又假設neuron取1000個，則一個100x100的圖片輸入，會產生$100<em>100</em>3(彩色)<em>1000 &#x3D; 3</em>10^7$個weight，是一個巨大的數字，大幅增加了overfitting的風險 (彈性過大)<br><img src="/../images/20220709_1.png"></li>
<li>不採用全連接層，以下透過一些觀察來嘗試簡化這個網路</li>
</ul>
<h2 id="Observation-1"><a href="#Observation-1" class="headerlink" title="Observation 1"></a>Observation 1</h2><ul>
<li>對於圖片辨識，我們要做的是針對圖片裡面找到一些關鍵的部位 (ex. 鳥嘴、眼睛、翅膀)</li>
<li>每個neuron並不需要看過整張圖片(即，不用fully connected)</li>
<li>我們可以讓每個neuron只看特定的區塊就好</li>
</ul>
<h3 id="Simplification-1-typical-settig"><a href="#Simplification-1-typical-settig" class="headerlink" title="Simplification 1 - typical settig"></a>Simplification 1 - typical settig</h3><ul>
<li><p>CNN會設定一個『Receptive Field』，每個neuron讀取一個他負責的區塊<br><img src="/../images/20220709_2.png"></p>
</li>
<li><p>Receptive Field可以重疊</p>
</li>
<li><p>Different neuron是可以有不同的receptive field的</p>
</li>
<li><p>上述案例裡面，就是3x3的kernel size</p>
</li>
<li><p>通常會有一排(64、128等)個neuron去守備他</p>
</li>
<li><p>不同的receptive field之間的距離差距稱為『stride』</p>
</li>
<li><p>通常receptive field都會高度重疊</p>
</li>
<li><p>如果一個receptive field關注的範圍超出圖片範圍，就需要把外面的值補值(補0、補平均等)，稱為『padding』</p>
</li>
<li><p>Receptive fields cover the whole image</p>
</li>
</ul>
<h2 id="Observation-2"><a href="#Observation-2" class="headerlink" title="Observation 2"></a>Observation 2</h2><ul>
<li>當一個特殊部位落在不同的receptive field內怎麼處理?</li>
</ul>
<h3 id="Simplification-2-typical-setting"><a href="#Simplification-2-typical-setting" class="headerlink" title="Simplification 2 -typical setting"></a>Simplification 2 -typical setting</h3><p><img src="/../images/20220709_3.png"></p>
<ul>
<li>我們可以讓一些neuron採用共用參數(Parameter sharing)，讓他們的參數都一模一樣</li>
<li>因為輸入(receptive field)不一樣，所以各自的輸出也不會相同</li>
<li>可能rf1的第一個neuron跟rf2的第一個neuron共用參數，rf1的第二個跟rf2的第二個neuron共參… etc</li>
<li>這些共用的參數稱為『Filter』<br><img src="/../images/20220709_4.png"></li>
</ul>
<h2 id="Benefit-of-Convolutional-layer"><a href="#Benefit-of-Convolutional-layer" class="headerlink" title="Benefit of Convolutional layer"></a>Benefit of Convolutional layer</h2><ul>
<li>根據上述的觀察，我們成功讓CNN network針對相片輸入的訓練更加簡化<br><img src="/../images/20220709_5.png"></li>
<li>Convolutional layer的model bias會比較大，但CNN是專門為影像設計的network<br>PS. 這邊為何CNN bias會比較大，以及為何這樣不好，可以再google一下</li>
</ul>
<hr>
<h1 id="另一個說明CNN的版本"><a href="#另一個說明CNN的版本" class="headerlink" title="另一個說明CNN的版本"></a>另一個說明CNN的版本</h1><h2 id="Convolutional-layer"><a href="#Convolutional-layer" class="headerlink" title="Convolutional layer"></a>Convolutional layer</h2><ul>
<li><p>所謂Convolutional Layer，裡面有很多的Filter，裡面都有一個3x3xchannel維的tensor</p>
</li>
<li><p>每一個filter都是要抓取某個pattern</p>
</li>
<li><p>以下假設是channel &#x3D; 1(黑白照片)</p>
</li>
<li><p>我們把各個rf跟filter做內積，得出各值<br><img src="/../images/20220709_6.png"></p>
</li>
<li><p>接下來把所有pattern對各filter一樣的計算</p>
</li>
<li><p>這內積出來的一群數字稱為『Feature map』，再這個例子中，我們有64個filter，則我們的feature map會有64組(channels)數字，每組有4x4個數字</p>
</li>
<li><p>接下來進到第二層的convolution，我們的filter必須變成3x3x<em>64</em>，因為上一層輸出了64個channel，相對於第一層只有一個channel，第二層會出現64個channel</p>
</li>
</ul>
<h4 id="Note"><a href="#Note" class="headerlink" title="Note:"></a>Note:</h4><ul>
<li>隨著捲積層的深入，我們觀察的圖片pattern會越來越大</li>
<li>繼續上面的例子，如果我們的filter之rf一樣是看3x3大小的話，因為我們的feature map中的3x3大小實際上是對應到圖片裡面的5x5大小(跟stride有關)，所以其實層數越高，我們一次考慮的範圍會越大！<br><img src="/../images/20220709_7.png"></li>
</ul>
<h2 id="Comparison-of-2-version"><a href="#Comparison-of-2-version" class="headerlink" title="Comparison of 2 version"></a>Comparison of 2 version</h2><ul>
<li>第一個版本的共用參數，就是第二版本的filter(本slide忽略bias)</li>
<li>把一個filter掃過一張圖片，稱作『convolves over』<ul>
<li>例句(?)： each filter convolves over the input image<br><img src="/../images/pasted-15.png"></li>
</ul>
</li>
</ul>
<p><img src="/../images/20220709_8.png"></p>
<h2 id="Observation-3"><a href="#Observation-3" class="headerlink" title="Observation 3"></a>Observation 3</h2><ul>
<li>如果我們把一張大圖片縮小、拿掉odd columns，圖片還是不會有所影響(看起來差不多)，稱為subsampling -&gt; pooling</li>
<li>Pooling 本身沒有參數，沒有任何東西要learn，有些人稱他為一種激發函數</li>
<li>pooling就是把圖片像素分組，然後從裡面只選一個像素留下，簡化圖片像素大小</li>
<li>下圖為示意圖<br><img src="/../images/20220709_9.png"></li>
<li>過度pooling仍會傷害訓練效益</li>
</ul>
<hr>
<h1 id="The-whole-CNN"><a href="#The-whole-CNN" class="headerlink" title="The whole CNN"></a>The whole CNN</h1><p><img src="/../images/20220709_10.png"></p>
<ul>
<li>做完卷積層以後要做flatten</li>
<li>flatten就是把矩陣數值拉直</li>
<li>flatten完以後扔進fully connected layer訓練完，配個softmax(分類)，就是一個經典的CNN network</li>
</ul>
<h1 id="Application-GO"><a href="#Application-GO" class="headerlink" title="Application: GO"></a>Application: GO</h1><ul>
<li><p>我們用一個19x19的向量來描述一個棋盤，把它扔進network以後輸出next move應該在的位置</p>
</li>
<li><p>下圍棋可以是一個類別分類問題</p>
</li>
<li><p>這個問題也可以用fully-connected network解決</p>
</li>
<li><p>但用CNN效果更好-&gt; 棋盤可以看做一個19x19來描述</p>
</li>
<li><p>每個棋盤格的channel有48個(這格可能被叫吃等等)</p>
</li>
<li><p>這意味著圍棋與影像有許多相似特性</p>
<ul>
<li>可以只看小區塊(alpha go: 5x5)<br>    - Same pattern appear in different regions (雙叫吃等等)</li>
</ul>
</li>
<li><p>棋盤可否用pooling ? 因為每格都很重要(精細度高) -&gt; Alpha Go有沒有用呢?
  </p>
</li>
<li><p>李宏毅教你畫重點XD：學著幫論文畫重點，抓critical terms<br><img src="/../images/20220709_11.png"></p>
</li>
<li><p>alpha go 正文沒有提到神經網路結構，這是在附件找到的</p>
<ol>
<li>視為19x19x48的image</li>
<li>zero pads(padding補0至23x23)</li>
<li>有k個filter(競賽用的go，filter &#x3D; 192)</li>
<li>filter的kernel size &#x3D; 5x5</li>
<li>stride &#x3D; 1</li>
<li>用到rectifier nonlinearity(ReLU)</li>
<li>2~12層都有做zero padding至21x21，filter數同，kernel size &#x3D; 3x3，stride &#x3D; 1</li>
<li>最後apply softmax function<br><img src="/../images/20220709_12.png"></li>
</ol>
</li>
<li><p>alpha go 沒有用pooling!!</p>
</li>
</ul>
<hr>
<h1 id="Hen重要的Notes"><a href="#Hen重要的Notes" class="headerlink" title="Hen重要的Notes:"></a>Hen重要的Notes:</h1><ul>
<li>語音上、文字處理上，文獻上的方法要仔細看，CNN的receptive field設計會特別為他們特化，這裡講的單純是影像的</li>
<li>CNN並不能處理影像放大縮小(Scaling)旋轉(Rotation)的問題… (向量問題)<ul>
<li>為此我們需要data augmentation</li>
</ul>
</li>
<li>其實有Network架構(Spatial Transformer Layer)可以解決這個問題，請Ref.<a target="_blank" rel="noopener" href="https://youtu.be/SoCywZ1hZak">這個影片</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一頁"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一頁"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">劉宇承</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 強力驅動
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>

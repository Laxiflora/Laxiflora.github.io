<!DOCTYPE html>
<html lang="zh-TW">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css" integrity="sha256-xejo6yLi6vGtAjcMIsY8BHdKsLg7QynVlFMzdQgUuy8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"laxiflora.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.12.3","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜尋...","empty":"我們無法找到任何有關 ${query} 的搜索結果","hits_time":"${hits} 找到 ${time} 個結果","hits":"找到 ${hits} 個結果"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="laxiflora的小天地">
<meta property="og:url" content="https://laxiflora.github.io/page/4/index.html">
<meta property="og:site_name" content="laxiflora的小天地">
<meta property="og:locale" content="zh_TW">
<meta property="article:author" content="劉宇承">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://laxiflora.github.io/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-TW","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>laxiflora的小天地</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切換導航欄" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">laxiflora的小天地</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">前進軟體工程師的練功之路</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>標籤</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>歸檔</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目錄
        </li>
        <li class="sidebar-nav-overview">
          本站概要
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">劉宇承</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">標籤</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="回到頂端">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/07/ML-2021-2-6-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%BA%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/07/ML-2021-2-6-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%BA%94/" class="post-title-link" itemprop="url">ML_2021_2-6 類神經網路訓練不起來怎麼辦(五)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-07 16:34:18" itemprop="dateCreated datePublished" datetime="2022-07-07T16:34:18+08:00">2022-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>簡短介紹Batch Normalization的技術</li>
<li>另一種直接改變error surface的技術(相對於動態lr，一種改善訓練的方法)</li>
</ul>
<p><img src="/../images/20220707_1.png" alt="upload successful"></p>
<ul>
<li>考慮以下模型，當$x_1$輸出很小、$x_2$輸出很大的時候，就會產生error surface橢圓的問題</li>
<li>因為$x_1$小，就算$w_1$變化很大，y的變化量也不會很大(因為相乘);$x_2$則相反</li>
<li>考慮可以把$x_1、x_2$相同的數值範圍<br><img src="/../images/20220707_2.png" alt="upload successful"></li>
</ul>
<h1 id="Feature-Normalization"><a href="#Feature-Normalization" class="headerlink" title="Feature Normalization"></a>Feature Normalization</h1><h2 id="其中一種normalization做法"><a href="#其中一種normalization做法" class="headerlink" title="其中一種normalization做法"></a>其中一種normalization做法</h2><ul>
<li>對多筆feature vector的同一dimention做標準化<br><img src="/../images/pasted-11.png"></li>
<li>標準化以後，該dim的平均值&#x3D;0，$\sigma$ &#x3D; 1</li>
<li>像這樣就可以製造比較平衡的error surface，方便optimization作業</li>
</ul>
<h2 id="Case：In-deep-learning"><a href="#Case：In-deep-learning" class="headerlink" title="Case：In deep learning"></a>Case：In deep learning</h2><ul>
<li>因為深度學習有多個層，雖然在一開始我們把x做了標準化，但是在經過一層layer計算以後，數值又失去了標準化，故我們需要進行多次的標準化</li>
<li>標準化要放在激發函數前後的影響並不大<br><img src="/../images/pasted-12.png"></li>
<li>以上圖為例，我們需要對z再度進行標準化，公式如下(feature&#x3D;3的case)<br><img src="/../images/20220707_4.png" alt="upload successful"><br>則可以得到<br>$$\tilde{z}^i &#x3D; \frac{z^i-\mu}{\sigma}$$<br>後續層也依此類推</li>
<li>這個feature標準化的過程使得所有feature之間有了關聯性 -&gt; 這是一個network<br><img src="/../images/20220707_5.png" alt="upload successful"></li>
</ul>
<h2 id="Case：training-in-batch-approach"><a href="#Case：training-in-batch-approach" class="headerlink" title="Case：training in batch approach?"></a>Case：training in batch approach?</h2><ul>
<li>這樣的標準化流程會跟著batch(一組batch內部做標準化)跑，不是所有feature納進來標準化</li>
<li>這樣的作法稱作batch normalization<ul>
<li>問題來了，我們會需要足夠大的batch size才能做一個好的標準化(誤差會比較小)<br><img src="/../images/20220707_6.png"></li>
</ul>
</li>
<li>$\beta、\gamma$是模型的另外兩個參數，透過學習得到</li>
<li>為啥需要這兩個參數?<ul>
<li>因為標準化會保證$\tilde{z}$之平均值 &#x3D; 0，這樣的結果有可能會對模型產生一些負面影響，所以我們需要$\beta、\gamma$兩個參數來讓數值變成比較貼合模型需求</li>
<li>問題：這樣不就又破壞掉標準化平衡了嗎?<ul>
<li>我們初始設定$\gamma &#x3D; 1 , \beta &#x3D; 0$，讓他們初始為真的標準化</li>
<li>讓模型來決定值該怎麼分步</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Batch-normalization-testing-data"><a href="#Batch-normalization-testing-data" class="headerlink" title="Batch normalization - testing data"></a>Batch normalization - testing data</h3><ul>
<li>剛剛講的都是training的情況下</li>
<li>testing又稱inference</li>
<li>當真的是線上模型時，我們必須每一筆資料進來就進行預測，不能用batch</li>
<li>當數據只有一筆，怎麼做normalization($\mu&#x3D;? , \sigma&#x3D;?$)</li>
<li>實作上的解法(pytorch)：<ul>
<li>在training若有用這個技術，每次batch算出來的$\mu_i , \sigma_i$就會記錄下來再做以下處理  <br><img src="/../images/pasted-13.png"></li>
</ul>
</li>
</ul>
<p>在實際test時，就代入<br>$$\tilde{z} &#x3D; \frac{z-\bar{u}}{\bar{\sigma}}<br>$$</p>
<ul>
<li>Batch normalization的實際測試結果，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167">連結</a><br><img src="/../images/20220707_7.png"></li>
<li>紅色線是有做batch normalization</li>
<li>粉色線，使用sigmoid function</li>
<li>其他線，就是lr乘上$x$倍</li>
<li>黑色沒有用BN，用inception</li>
<li>收斂速度更快，但結果差不多</li>
</ul>
<h2 id="How-does-Batch-Normalization-help-optimization"><a href="#How-does-Batch-Normalization-help-optimization" class="headerlink" title="How does Batch Normalization help optimization?"></a>How does Batch Normalization help optimization?</h2><ul>
<li><p>下面這篇論文的作者發明這個詞”Internal covariate shift”</p>
</li>
<li><p>根據<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.11604">這篇論文</a>所認為有以下可能<br><img src="/../images/pasted-14.png"></p>
</li>
<li><p>我們做參數update，將A變成A’，B變成B’，但是B的變動是根據之前算出來的a作為input，當整體更新了以後，B’要面對的input卻不再是a，而是經過A’算出來的a’，故導致仍舊失準</p>
<ul>
<li>而Batch normalization的作法，是讓a跟a’有相似的分布(similar statistics)，故誤差會比較接近</li>
</ul>
</li>
<li><p>但是Experimental result並不支持這個緣故(打臉)</p>
<ul>
<li>打臉者認為實驗下來，a跟a’的分布都差不多，而且不管分布是不是差很多，影響都不大，於是這個假說是錯的(不是batch normalization的關鍵)</li>
<li>不過實驗跟理論依然證明，Batch normalization依然會改變error surface的地貌<br><img src="/../images/20220707_9.png"></li>
<li>此人認為batch normalization的發現可能是偶然(意料之外)的，但無論如何這是有用的方法<br>    - normalization有一堆方法，參考如下<br><img src="/../images/20220707_10.png"></li>
</ul>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/06/ML-2021-2-5-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E5%9B%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/06/ML-2021-2-5-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E5%9B%9B/" class="post-title-link" itemprop="url">ML_2021_2-5 類神經網路訓練不起來怎麼辦(四)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-06 22:20:08" itemprop="dateCreated datePublished" datetime="2022-07-06T22:20:08+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>這是如何分類的短版本，長版本連結如下:<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=fZAZUYEeIMg">連結1</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=hSXFuypLukA">連結2</a></li>
<li>這裡直接講分類是怎麼做的</li>
</ul>
<h1 id="Classification-as-Regression"><a href="#Classification-as-Regression" class="headerlink" title="Classification as Regression?"></a>Classification as Regression?</h1><p>Regression長這樣<br><img src="/../images/pasted-7.png"></p>
<ul>
<li>那classification怎麼看得像regression呢？</li>
<li>我們讓輸出的y(原本是一種類別)變成編號，跟$\hat{y}$比對</li>
<li>但這樣會有問題，class 1 跟 class 2 也不同類(loss &#x3D; 1)，但他們的loss會小於class 1 跟 class 3的錯誤預測(loss &#x3D; 2)</li>
<li>常見的做法是把class用one-hot vector來表示</li>
<li>當然，這樣我們就會希望output的y是一個向量而非純量 $\rightarrow$ 我們可以用多組的weight去做多次輸出，如下圖<br><img src="/../images/pasted-8.png"></li>
<li>我們通常會算出一個y以後，先做一個softmax(y)得到y’才去比較</li>
</ul>
<h2 id="Softmax-activate-function"><a href="#Softmax-activate-function" class="headerlink" title="Softmax activate function"></a>Softmax activate function</h2><p>公式如下<br>$$<br>y_i’ &#x3D; \frac{exp(y_i)}{\sum_{j}exp(y_i)}<br>$$<br>圖例<br><img src="/../images/pasted-9.png"><br>softmax有兩個特徵：<br>$$<br>1&gt;y_i&gt;0 \\\<br>\sum_iy_i’ &#x3D; 1<br>$$</p>
<ul>
<li>其實就是把$y_i$的各自機率算出來，若y&lt;0則機率~0</li>
</ul>
<h2 id="why-add-softmax-at-last-layer-in-classification"><a href="#why-add-softmax-at-last-layer-in-classification" class="headerlink" title="why add softmax at last layer in classification?"></a>why add softmax at last layer in classification?</h2><ul>
<li>可以參考原版錄影，因為解釋較長</li>
<li>騙小孩的說法是，因為機率是0到1之間，所以我們可以把y做softmax讓他normalize</li>
</ul>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note :"></a>Note :</h3><ul>
<li>Sigmoid就是2 class版的softmax(Ref.深度學習的數學地圖)</li>
</ul>
<h1 id="分類模型的loss-function"><a href="#分類模型的loss-function" class="headerlink" title="分類模型的loss function"></a>分類模型的loss function</h1><ul>
<li>我們仍然可以採用MSE來計算</li>
<li>但更常用的作法是用cross entropy</li>
</ul>
<h2 id="cross-entropy-loss-function"><a href="#cross-entropy-loss-function" class="headerlink" title="cross entropy loss function"></a>cross entropy loss function</h2><p>公式如下<br>$$<br>e &#x3D; - \sum_i\hat{y}_ilny_i’<br>$$</p>
<ul>
<li>最小值就是當$y &#x3D; \hat{y}$</li>
<li>minimize cross entropy &#x3D; maximize likelihood</li>
<li>基本上softmax是被跟cross entropy綁在一起的，因為向性很高</li>
<li>所以如果用cross entropy當loss，那模型最後一層自動就會補上softmax當激發函數(pytorch)</li>
</ul>
<h2 id="用optimizer的角度來證實cross-entropy優於MSE"><a href="#用optimizer的角度來證實cross-entropy優於MSE" class="headerlink" title="用optimizer的角度來證實cross entropy優於MSE"></a>用optimizer的角度來證實cross entropy優於MSE</h2><ul>
<li><a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/Deep%20More%20(v2).ecm.mp4/index.html">用數學證明的方式說明</a>請參考過去影片</li>
<li>以下用舉例的方式說明</li>
<li>已知一個模型如下圖<br><img src="/../images/pasted-10.png"><br>則MSE跟cross entropy的表現如下</li>
</ul>
<p><img src="/../images/20220706_15.png" alt="upload successful"></p>
<ul>
<li><p>用MSE的前提下，因為點就卡在高loss了，周圍很平坦，很難用梯度下降找到更好的點</p>
</li>
<li><p>這是一個透過改變loss function來改變整個error surface的例子</p>
</li>
<li><p>loss function的定義是有可能影響訓練難度的</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/06/ML-2021-2-4-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%B8%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/06/ML-2021-2-4-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%B8%89/" class="post-title-link" itemprop="url">ML_2021_2-4 類神經網路訓練不起來怎麼辦(三)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-06 14:04:41" itemprop="dateCreated datePublished" datetime="2022-07-06T14:04:41+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li>Critical point 不一定是在訓練模型時會碰到的最大問題</li>
</ul>
<h1 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h1><ul>
<li>我們都認為training loss卡住了之後，是因為parameters卡在critical point</li>
<li>其實有可能是loss function在兩個谷間碰撞，可能是兩組parameters之間剛好loss差不多</li>
<li>考慮一個情境，高爾夫球一直在球洞兩邊滾來滾去，就是滾不到終點<br><img src="/../images/pasted-3.png"></li>
<li>多數training其實還沒走到critical point就已經停止 (所以真正要注意的點不是critical point)</li>
</ul>
<h2 id="非卡在critical-point的Example"><a href="#非卡在critical-point的Example" class="headerlink" title="非卡在critical point的Example"></a>非卡在critical point的Example</h2><ul>
<li><p>給定一個convex error surface，如下圖<br><img src="/../images/20220706_2.png" alt="upload successful"></p>
</li>
<li><p>當learning rate太大，會容易在等高線密集的地方邁步過大，如下圖<br><img src="/../images/20220706_3.png" alt="upload successful"></p>
</li>
<li><p>或是當learning rate太小，容易卡在低谷幾乎動不了(要挪到X需要好幾百萬次更新)，如下圖<br><img src="/../images/image.png" alt="upload successful"></p>
</li>
</ul>
<p>$\rightarrow$ 單一learning rate通常不能貫徹模型訓練的整個過程</p>
<h2 id="如何設定learning-rate-Adagrad-Approach"><a href="#如何設定learning-rate-Adagrad-Approach" class="headerlink" title="如何設定learning rate? - Adagrad Approach"></a>如何設定learning rate? - Adagrad Approach</h2><ul>
<li><p>從上個例子可知，當某方向上等高線密集時，我們需要learning rate 小，反之則要大</p>
</li>
<li><p>為了讓$\eta$能自動變動，要調整公式</p>
</li>
<li><p>下圖為梯度下降法的原始公式<br><img src="/../images/20220706_4.png" alt="upload successful"></p>
</li>
<li><p>方便起見，這裡只用一個參數</p>
</li>
<li><p>更動以後的算式如下</p>
</li>
</ul>
<p>$$<br>\theta^{t+1}_i \leftarrow \theta^t_i - \frac{\eta}{\sigma^t_i} g^t_i<br>$$</p>
<ul>
<li>我們讓$\sigma^t_i$加入等式，這樣就可以讓learning rate變成一個parameter dependent的hyper parameter</li>
</ul>
<h3 id="如何計算sigma"><a href="#如何計算sigma" class="headerlink" title="如何計算sigma?"></a>如何計算sigma?</h3><p>$$<br>\theta^{1}_i \leftarrow \theta^0_i - \frac{\eta}{\sigma^0_i} g^0_i<br>$$<br>其中$\sigma^0_i &#x3D; \sqrt{(g^0_i)^2} &#x3D; \vert g^0_i \vert$<br>接下來<br>$$<br>\theta^{2}_i \leftarrow \theta^1_i - \frac{\eta}{\sigma^0_i} g^1_i<br>$$<br>其中$\sigma^1_i &#x3D; \sqrt{\frac{1}{2}[(g^0_i)^2+(g^1_i)^2]}$</p>
<p>…<br>一路推廣，可以得到</p>
<p>$$<br>\theta^{t+1}_i \leftarrow \theta^t_i - \frac{\eta}{\sigma^t_i} g^t_i<br>$$</p>
<p>其中</p>
<p>$$<br>\sigma^t_i &#x3D; \sqrt{\frac{1}{t+1}\sum_{j&#x3D;0}^{t}(g_i^j)^2}<br>$$</p>
<ul>
<li>目前這個技巧應用在Adagrad</li>
</ul>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><ul>
<li>當gradient小(平坦)，算出來的$\sigma$就小，learning rate大</li>
<li>當gradient大(陡峭)，算出來的$\sigma$就大，learning rate小<br><img src="/../images/pasted-4.png"></li>
</ul>
<h3 id="這樣會有甚麼問題"><a href="#這樣會有甚麼問題" class="headerlink" title="這樣會有甚麼問題"></a>這樣會有甚麼問題</h3><ul>
<li>剛才的假設是同一個參數，他的gradient大小就固定一個值(?)</li>
<li>就算是同一個參數，他需要的learning rate也會隨時間而改變</li>
<li>我們期待就算是同一個參數在同一個方向，learning rate也會有所改變<br>舉例，我們討論橫軸</li>
</ul>
<p><img src="/../images/20220706_5.png" alt="upload successful"></p>
<hr>
<h2 id="如何設定learning-rate-RMSProp-Approach"><a href="#如何設定learning-rate-RMSProp-Approach" class="headerlink" title="如何設定learning rate? - RMSProp Approach"></a>如何設定learning rate? - RMSProp Approach</h2><ul>
<li>一個沒有論文的方法orz</li>
<li>方法如下圖<br><img src="/../images/pasted-5.png"></li>
<li>主要改變了紅圈圈起來的部分，捨棄了用前面所有的gradient求MSE決定$\sigma$的方法，RMSProp只採計上一個$\sigma$值以及這次的gradient之MSE和</li>
<li>多了一個hyper parameter $alpha$，調整對上一個$\sigma$的學習率高低</li>
<li>其實上一個$\sigma$就包含了前面所有的gradient之MSE，只是權重會隨著疊代越來越小</li>
</ul>
<p>learning rate變動範例圖<br><img src="/../images/20220706_7.png"></p>
<h2 id="回到一開始的範例"><a href="#回到一開始的範例" class="headerlink" title="回到一開始的範例"></a>回到一開始的範例</h2><ul>
<li>[回到這個範例](# 非卡在critical point的Example)，我們來看看各approach的效果</li>
</ul>
<h4 id="Adaptive-learning-rate"><a href="#Adaptive-learning-rate" class="headerlink" title="Adaptive learning rate"></a>Adaptive learning rate</h4><p><img src="/../images/pasted-6.png" alt="filename already exists, renamed"></p>
<ul>
<li>為啥爆炸了?<ul>
<li>根據公式，我們把前面幾次的gradient都列入計算，因為在橫線的部分步伐很大，所以當走到步伐該縮小的時候，會爆衝<br>    - 但也因為公式，爆衝一陣子以後learning rate會逐漸縮小，然後回歸正軌，等待一陣子以後learning rate上升再度爆炸</li>
</ul>
</li>
</ul>
<h5 id="解法：learning-rate-decay"><a href="#解法：learning-rate-decay" class="headerlink" title="解法：learning rate decay"></a>解法：learning rate decay</h5><ul>
<li>隨著訓練的進行，我們一定越來越接近終點</li>
<li>可以隨著時間降低learning rate，開始微調</li>
</ul>
<h5 id="解法2：Warm-up"><a href="#解法2：Warm-up" class="headerlink" title="解法2：Warm up"></a>解法2：Warm up</h5><p><img src="/../images/20220706_10.png"></p>
<ul>
<li>算是一種黑科技</li>
<li>先變大learning rate，再縮小(?)</li>
<li>在訓練bert的時候常常用到，但他在很久以前就出現在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">論文</a>過了</li>
<li>在transformer中也出現過，見下圖<br><img src="/../images/20220706_11.png"></li>
<li>一種可能的解釋是，因為$\sigma$是統計的數據，在訓練初期的時候容易失準，故初期讓learning rate小，等到$\sigma$精準一點以後，再讓learning rate變高</li>
<li>相關paper : <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.03265">RAdam</a></li>
</ul>
<h2 id="Summary-of-Optimization"><a href="#Summary-of-Optimization" class="headerlink" title="Summary of Optimization"></a>Summary of Optimization</h2><p><img src="/../images/20220706_12.png"></p>
<ul>
<li>雖然Momentum跟$\sigma$都使用過去的資料，但不會因此抵銷<ul>
<li>Momentum是把所有gradient加起來，故有考慮方向與正負號</li>
<li>$\sigma$只考慮MSE</li>
</ul>
</li>
</ul>
<h2 id="下次預告"><a href="#下次預告" class="headerlink" title="下次預告"></a>下次預告</h2><ul>
<li>當訓練過程遭遇大山，要如何闢路繞過去？</li>
<li>有沒有可能直接炸掉大山，改變error surface呢?<br>$\rightarrow$ Batch normalization<br>PS. 課程跳到2-6哦</li>
</ul>
<hr>
<h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><ul>
<li><p>現今最常見的Optimizer:Adam其實就是RMSProp + Momentum</p>
</li>
<li><p>Adam的細節自行參考</p>
</li>
<li><p>arxiv論文年代看法</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385%E7%82%BA%E4%BE%8B">https://arxiv.org/abs/1512.03385為例</a><br>    - 15代表2015年出版<br>    - 12代表12月</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/05/ML-2021-2-3-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%BA%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/05/ML-2021-2-3-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%BA%8C/" class="post-title-link" itemprop="url">ML_2021_2-3 類神經網路訓練不起來怎麼辦(二)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-05 21:59:49" itemprop="dateCreated datePublished" datetime="2022-07-05T21:59:49+08:00">2022-07-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><ul>
<li>之前課程(Ref. 2021版1-1)說到，在實際train data時我們不是實際把所有Data算出來對L微分求loss，而是把Data先分組切成batch，而所有batch看過一遍，才叫1 epoch</li>
<li>我們可以設定shuffle，就是每個不同的epoch，data都會重新分組</li>
</ul>
<h1 id="Batch-size對模型之影響"><a href="#Batch-size對模型之影響" class="headerlink" title="Batch size對模型之影響"></a>Batch size對模型之影響</h1><ul>
<li>首先比較兩個case：<br><img src="/../images/20220705_2.png"></li>
<li>左邊沒有用batch，而右邊batch size &#x3D; 1</li>
<li>左邊表示每次必須看完所有data，參數才能更新一次；右邊則相反，每看完一個data就更新一次參數</li>
<li>一個是重攻擊長CD，一個是輕攻擊短CD (?)</li>
</ul>
<h2 id="大batch優缺"><a href="#大batch優缺" class="headerlink" title="大batch優缺"></a>大batch優缺</h2><h5 id="優點"><a href="#優點" class="headerlink" title="優點"></a>優點</h5><ul>
<li>威力大(更新幅度大)且每步都很穩</li>
</ul>
<h5 id="缺點"><a href="#缺點" class="headerlink" title="缺點"></a>缺點</h5><ul>
<li>超級慢<br>(小batch優缺則顛倒)</li>
</ul>
<h2 id="small-batch-or-big-batch"><a href="#small-batch-or-big-batch" class="headerlink" title="small batch or big batch?"></a>small batch or big batch?</h2><ul>
<li>兩邊看似各自相對，但我們還沒考慮平行運算</li>
<li>若考慮多核心，其實大batch不會比較慢</li>
<li>以下圖為例，用tesla V100 GPU，batch size到1000都很合適(梯度法)<br><img src="/../images/20220705_1.png"></li>
<li>因為高batch不一定更花時間，所以其實在平行運算下，大batch的每epoch速度甚至還比小epoch還快</li>
<li>不過，就算是平行運算下，大batch一定比較好嗎?<ul>
<li>與直覺相反的是，noisy大有時候反而可以促進訓練能力(如下圖)</li>
</ul>
</li>
</ul>
<p><img src="/../images/20220705_3.png" alt="upload successful"></p>
<ul>
<li>相同model(同bias)下，smaller batch size有更高的performance</li>
<li>問題來源於optimizer fails(Ref. 2021版2-2關於如何判定問題是哪種)</li>
</ul>
<h2 id="為何small-batch-size可以有更好的結果？"><a href="#為何small-batch-size可以有更好的結果？" class="headerlink" title="為何small batch size可以有更好的結果？"></a>為何small batch size可以有更好的結果？</h2><ul>
<li>一說是Noisy update is better for training<br>見下圖</li>
</ul>
<p><img src="/../images/pasted-1.png"><br>small batch下，或許$L^1$卡住了，但是因為$L^2$跟$L^1$有不小的差距，所以剛好可以讓參數更新然後把參數撞開critical point</p>
<h3 id="small-batch-still-better-for-testing-data"><a href="#small-batch-still-better-for-testing-data" class="headerlink" title="small batch still better for testing data?"></a>small batch still better for testing data?</h3><ul>
<li>我們知道了small batch在training set表現更好，那testing data是否也是如此呢?</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04836">根據這篇論文</a>，可以知道small batch在測試資料也有好表現</li>
</ul>
<p><img src="/../images/20220705_4.png" alt="upload successful"></p>
<ul>
<li>就算能把big batch model的準確率練到跟small batch model差不多(調hyper parameter)，在測試時也會有差距顯現 $\rightarrow$ big batch有overfitting問題<br>目前普遍認為的原因如下：<ul>
<li>就算是卡在local minima，也有分好壞</li>
<li>周圍平坦的local minima較好</li>
<li>而論文認為大batch size傾向會讓我們走向尖銳的minima point<ul>
<li>一種說法是，小batch size更動頻繁走向豐富，容易走出周圍很高的minima point(對於逃離critical point能力較高)，而停在平坦的minima point (Ref. 2021版2-2 鞍點)</li>
</ul>
<p>	</p>
</li>
<li>因為testing 跟 training的loss function，肯定會有所小偏差，如下圖<br><img src="/../images/20220705_6.png" alt="upload successful"></li>
</ul>
</li>
</ul>
<h2 id="Summary：Batch-comparasion"><a href="#Summary：Batch-comparasion" class="headerlink" title="Summary：Batch comparasion"></a>Summary：Batch comparasion</h2><p><img src="/../images/20220705_7.png" alt="upload successful"></p>
<ul>
<li>因此，batch size最終成為了一個hyper parameter</li>
</ul>
<h2 id="魚與熊掌兼得的辦法"><a href="#魚與熊掌兼得的辦法" class="headerlink" title="魚與熊掌兼得的辦法?"></a>魚與熊掌兼得的辦法?</h2><ul>
<li>看ref啃論文  &#x3D; &#x3D;</li>
</ul>
<hr>
<h1 id="另一個對抗saddle-point或minima的技術-momentum"><a href="#另一個對抗saddle-point或minima的技術-momentum" class="headerlink" title="另一個對抗saddle point或minima的技術 : momentum"></a>另一個對抗saddle point或minima的技術 : momentum</h1><ul>
<li>當small gradient的時候，我們走到一個鞍點或局部最小點之後就會停下來</li>
<li>但我們可以用物理式來思考，球由高處滾下來到最低點碰到上坡，不一定會停下來，因為有動能</li>
</ul>
<h2 id="Review-原本的梯度下降"><a href="#Review-原本的梯度下降" class="headerlink" title="Review : 原本的梯度下降"></a>Review : 原本的梯度下降</h2><p><img src="/../images/pasted-2.png"></p>
<h2 id="梯度下降-momentum"><a href="#梯度下降-momentum" class="headerlink" title="梯度下降+momentum"></a>梯度下降+momentum</h2><ul>
<li>簡單來說，就是加入慣性，額外參考上一次的移動軌跡</li>
</ul>
<p><img src="/../images/20220706_8.png" alt="upload successful"></p>
<ul>
<li>多了一個hyper parameter $\lambda$<br><img src="/../images/20220706_9.png" alt="upload successful"></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/04/ML-2022-2-1-%E5%86%8D%E6%8E%A2%E5%AF%B6%E5%8F%AF%E5%A4%A2%E3%80%81%E6%95%B8%E7%A2%BC%E5%AF%B6%E8%B2%9D%E5%88%86%E9%A1%9E%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/04/ML-2022-2-1-%E5%86%8D%E6%8E%A2%E5%AF%B6%E5%8F%AF%E5%A4%A2%E3%80%81%E6%95%B8%E7%A2%BC%E5%AF%B6%E8%B2%9D%E5%88%86%E9%A1%9E%E5%99%A8/" class="post-title-link" itemprop="url">ML_2022_2-1 再探寶可夢、數碼寶貝分類器</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-04 22:24:04" itemprop="dateCreated datePublished" datetime="2022-07-04T22:24:04+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/04/ML-2021-2-2-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%B8%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/04/ML-2021-2-2-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%B8%80/" class="post-title-link" itemprop="url">ML_2021_2-2 類神經網路訓練不起來怎麼辦(一)</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-04 22:22:35" itemprop="dateCreated datePublished" datetime="2022-07-04T22:22:35+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <ul>
<li><p>這裡只討論optimazion失靈的時候，如何把梯度下降做得更好</p>
<h2 id="為何opti會失敗（grad）？"><a href="#為何opti會失敗（grad）？" class="headerlink" title="為何opti會失敗（grad）？"></a>為何opti會失敗（grad）？</h2><ul>
<li>gradient decent&#x3D;0，使參數無法再更新</li>
<li>點卡在local minima or saddle point(稱為卡在critical point)</li>
</ul>
<h3 id="分辨critical-point是saddle-point-or-local-minima"><a href="#分辨critical-point是saddle-point-or-local-minima" class="headerlink" title="分辨critical point是saddle point or local minima"></a>分辨critical point是saddle point or local minima</h3><ul>
<li>雖然我們無法知道loss function長怎樣，但可以用泰勒展開式逼近</li>
<li>給定一組參數$\theta’$</li>
<li>則使用tayler series approximation<br>原式為<br>$$<br>f(x) &#x3D; f(a) + \frac{f’(a)}{1!}(x-a) + \frac{f’’(a)}{2!}(x-a)^2 + ….<br>$$<br>我們取到2次微分，代入$x &#x3D; \theta及a &#x3D; \theta’$得到<br>$$<br>L(\theta) \sim L(\theta’)^Tg + \frac{1}{2}(\theta - \theta’)^TH(\theta - \theta’)<br>,其中 g &#x3D; \nabla L(\theta’)(請參考上週)<br>$$</li>
</ul>
</li>
</ul>
<p><img src="/../images/20220704_1.png"></p>
<ul>
<li><p>其實g就是$L(\theta’)$對$\theta_i$的一階微分，Hessian是$L(\theta’)$對$\theta_{ij}$做二次微分</p>
</li>
<li><p>因為critical point的時候g &#x3D; 0，所以我們要考慮H(也就是2次微分的部分)來分辨是哪種問題，2次微分可以看出地貌<br>把Hessian部分拉出來討論<br><img src="/../images/20220704_3.png"><br>要如何確認滿足哪個原因呢?</p>
<ul>
<li>線性代數：正定、負定、均非 (看H的eigen value)</li>
</ul>
<h3 id="範例說明"><a href="#範例說明" class="headerlink" title="範例說明"></a>範例說明</h3></li>
<li><p>給定一個模型$y &#x3D; w_1w_2x$，資料僅有一筆，$f(1)$時其label &#x3D; 1，且w1w2之間不具有任何激發函數，loss function採用MSE</p>
</li>
<li><p>則x&#x3D;1時透過爆搜我們可以得出下面的error surface(偷偷看正解圖)<br><img src="/../images/20220704_2.png"></p>
</li>
<li><p>其中鞍點的四周都是高牆，無法離開</p>
</li>
<li><p>局部最小點則是在範圍內找不到更低的點</p>
</li>
<li><p>但假設我們不知道這個error surface，我們可以應用上面的方法來測定他是哪個問題，根據MSE公式我們得出<br>$$<br>L &#x3D; (\hat{y}-w_1w_2x)^2 &#x3D; (1-w_1w_2)^2<br>$$<br>對他們做微分可以得到(注意chain rule)<br>$$<br>g &#x3D; \frac{\partial L}{\partial w_1} &#x3D; 2(1-w_1w_2)(-w_2)<br>$$<br>$$<br>g &#x3D; \frac{\partial L}{\partial w_2} &#x3D; 2(1-w_1w_2)(-w_1)<br>$$<br>代入g&#x3D;0，可以發現當$w_1 &#x3D; w_2 &#x3D; 0$，有critical point<br>接下來要確認他們是哪個問題，就繼續再做微分:<br><img src="/../images/20220704_5.png"><br>代入剛剛的$w_1 &#x3D; w_2 &#x3D; 0$得到<br>$$<br>H &#x3D;  <br>\left[<br>\begin{matrix}<br>     0 &amp;&amp; -2 \\\ -2 &amp;&amp; 0<br>\end{matrix}<br>\right] \tag{3}<br>$$<br>抓H的eigen values來知道他是哪個point</p>
</li>
</ul>
<h2 id="case-saddle-point"><a href="#case-saddle-point" class="headerlink" title="case: saddle point"></a>case: saddle point</h2><ul>
<li>我們可以藉由H來得到參數該移動的方向<br>令$\lambda$是H的一個eigen value, u為$\lambda$的其中一個eigen vector<br>則<br>$$<br>v^THv &#x3D; u^THu &#x3D; u^T(\lambda u) &#x3D; \lambda \vert \vert u \vert \vert^2<br>$$<br>若今天$\lambda &lt; 0$則必定$u^THu&lt;0$，回顧剛剛的式子<br>$$<br>L(\theta) \sim L(\theta’)^Tg + \frac{1}{2}(\theta - \theta’)^TH(\theta - \theta’)<br>$$<br>可以知道$L(\theta)必定&gt;L(\theta’)$<br><img src="/../images/20220704_6.png"><br>我們只要將$\theta’沿著u的方向更新u得到\theta$，就可以再次降低loss</li>
</ul>
<h3 id="範例說明-1"><a href="#範例說明-1" class="headerlink" title="範例說明"></a>範例說明</h3><ul>
<li>延續剛剛的例子，我們知道$H的\lambda_1 &#x3D; 2, \lambda_2 &#x3D;-2$，屬於saddle point(非正定與負定)</li>
<li>取$\lambda_2 &#x3D; -2 , u &#x3D; (1,1)$，則把$\theta &#x3D; (0,0)+(1,1)$，就可以逃出saddle point</li>
<li>實務上不易使用，因為要做出2次微分且還需要用到找出該矩陣的eigen value，計算量過大 (還有別招可以用)</li>
</ul>
<h2 id="Saddle-point-vs-Local-Minima"><a href="#Saddle-point-vs-Local-Minima" class="headerlink" title="Saddle point vs. Local Minima"></a>Saddle point vs. Local Minima</h2><ul>
<li>在三維的密閉石棺中，在更高維度未必是密閉的</li>
<li>在低維度的local minima中，是否只是高維中的saddle point? </li>
<li>當參數超級多，是否極度有可能local minima其實只是saddle point? (假說)</li>
<li>在實作中，絕大多數的模型，critical point所在點中，幾乎找不到所有eigen value均&gt;0的範例，表示我們幾乎不可能找到完全的local minima</li>
<li>定義一個數值”Minimum ratio &#x3D; $\frac{正\lambda數}{\lambda數}$”，表示你的critical point有多像local minima</li>
<li>所以我們可以知道，通常一個模型train到loss卡住，極高可能是卡在一個saddle point</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/04/ML-2021-2-1-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E4%BB%BB%E5%8B%99%E6%94%BB%E7%95%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/04/ML-2021-2-1-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E4%BB%BB%E5%8B%99%E6%94%BB%E7%95%A5/" class="post-title-link" itemprop="url">ML_2021_2-1 機器學習任務攻略</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-04 22:20:49" itemprop="dateCreated datePublished" datetime="2022-07-04T22:20:49+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="機器學習任務攻略"><a href="#機器學習任務攻略" class="headerlink" title="機器學習任務攻略"></a>機器學習任務攻略</h2><p><img src="/../images/202200703_7.png"><br>  - 當有一個模型的表現不佳，有時候不一定是overfitting的問題<br>  - 問題來源可能有數種<br>    - Model Bias：模型本身的彈性不足，值域過小導致找不到最佳解<br>    - Optimization Issue：模型的彈性是夠的，也就是$f^*(x)$存在，只是因為optimizer不給力，始終無法把$\theta$帶到loss更小的地方</p>
<h2 id="如何分辨是model-bias-issue-or-Optimization-issue"><a href="#如何分辨是model-bias-issue-or-Optimization-issue" class="headerlink" title="如何分辨是model bias issue or Optimization issue?"></a>如何分辨是model bias issue or Optimization issue?</h2><p>  - 以下圖為例<br><img src="/../images/20220703_1.png"></p>
<ul>
<li>通常我們會認為這是模型overfitting了，才會導致層數增加反倒命中率下降</li>
<li>overfitting固然有可能，但它不是唯一的可能性，我們應該要從訓練資料的loss下手<br><img src="/../images/20220703_2.png"></li>
<li>可以發現就算是訓練資料，56層的loss也是大於20層的，故排除overfitting的可能</li>
<li>至於模型彈性這個可能也可以排除，因為56層的複雜度&gt;20層，故56層的loss只可能比20層還小，若是顛倒的話表示應該不是模型彈性問題，而是optimizer的鍋</li>
</ul>
<hr>
<ul>
<li>若是發現類似優化器的問題，可以試試看先跑一些比較淺層的model或是簡易的model(如linear regression)，先觀察得出來的結果</li>
<li>再跑深度的model，比較他們的結果，若深度的結果沒比較好，可以考慮換optimizer<br>原因樹狀圖(?<br><img src="/../images/20220703_3.png"></li>
</ul>
<h2 id="Overfitting的成因"><a href="#Overfitting的成因" class="headerlink" title="Overfitting的成因"></a>Overfitting的成因</h2><p><img src="/../images/20220703_4.png"></p>
<ol>
<li>若訓練資料不足，模型彈性過高也可能導致overfitting(增加training data最有效)<ul>
<li>data augmentation也是一種方法(ex.圖片翻轉作為新資料)</li>
</ul>
</li>
<li>降低模型的彈性也可以降低這個可能</li>
<li>採用一些技巧<ul>
<li>Early stopping</li>
<li>Regularization</li>
<li>Dropout</li>
<li>Less features</li>
</ul>
</li>
</ol>
<h2 id="衡量模型"><a href="#衡量模型" class="headerlink" title="衡量模型"></a>衡量模型</h2><ul>
<li>或許某個model在所有model裡面test成績最突出，但它未必會真的是最好的模型(運氣問題)<br><img src="/../images/20220703_5.png"></li>
<li>極端範例，一個模型剛好隨機出了最好的結果</li>
<li>所以testing set有分public跟private，避免一直上傳模型賭出最好成績</li>
</ul>
<h3 id="N-fold-Cross-Validation"><a href="#N-fold-Cross-Validation" class="headerlink" title="N-fold Cross Validation"></a>N-fold Cross Validation</h3><ul>
<li>把train set切成3份，2份是training data，1份是val data</li>
<li>交叉身分去訓練n次</li>
</ul>
<p><img src="/../images/20220703_6.png" alt="upload successful"></p>
<h4 id="data-mismatch"><a href="#data-mismatch" class="headerlink" title="data mismatch"></a>data mismatch</h4><ul>
<li>training data 跟 testing data有不同分布</li>
<li>ex. 機器學習2020年觀看人數預測2021年觀看人數，很高可能會mismatch</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/04/ML-2022-1-1-%E6%AD%A3%E8%AA%B2%E5%85%A7%E5%AE%B9%E4%BB%8B%E7%B4%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/04/ML-2022-1-1-%E6%AD%A3%E8%AA%B2%E5%85%A7%E5%AE%B9%E4%BB%8B%E7%B4%B9/" class="post-title-link" itemprop="url">ML_2022_1-1 正課內容介紹</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-04 22:19:15" itemprop="dateCreated datePublished" datetime="2022-07-04T22:19:15+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="正課內容介紹"><a href="#正課內容介紹" class="headerlink" title="正課內容介紹"></a>正課內容介紹</h1><h3 id="Supervised-Learning"><a href="#Supervised-Learning" class="headerlink" title="Supervised Learning"></a>Supervised Learning</h3><h3 id="Self-supervised-Learning"><a href="#Self-supervised-Learning" class="headerlink" title="Self-supervised Learning"></a>Self-supervised Learning</h3><ul>
<li>在訓練之前先做基本功(Pre-Train)</li>
<li>訓練集就不需要label了</li>
<li>比如在網路上先爬相關資料</li>
<li>以分類圖片為例，實際分類圖片的過程稱為(Downstream)</li>
<li>Pre-trained model(又稱Foundation Model)就好像OS，Downstream就像APP<ul>
<li>最出名的模型就是BERT</li>
</ul>
</li>
</ul>
<h3 id="Lec6-GAN"><a href="#Lec6-GAN" class="headerlink" title="Lec6 : GAN"></a>Lec6 : GAN</h3><ul>
<li>一般函數需要大量成對的X與Y來讓他們配對</li>
<li>現今只需要大量的X，就可自行產生Y</li>
</ul>
<h3 id="Lac12-Reinforcement-Learning"><a href="#Lac12-Reinforcement-Learning" class="headerlink" title="Lac12 : Reinforcement Learning"></a>Lac12 : Reinforcement Learning</h3><ul>
<li>當資料很難以標註時使用 (ex.學會下圍棋)</li>
</ul>
<h2 id="進階課題"><a href="#進階課題" class="headerlink" title="進階課題"></a>進階課題</h2><h3 id="Lac8-Anomaly-Detection"><a href="#Lac8-Anomaly-Detection" class="headerlink" title="Lac8 : Anomaly Detection"></a>Lac8 : Anomaly Detection</h3><ul>
<li>過擬合等問題</li>
</ul>
<h3 id="Lac9-Explainable-AI-可解釋性AI"><a href="#Lac9-Explainable-AI-可解釋性AI" class="headerlink" title="Lac9 : Explainable AI (可解釋性AI)"></a>Lac9 : Explainable AI (可解釋性AI)</h3><ul>
<li>不只是知道答案，還要讓機器知道為甚麼</li>
<li>EX.PNG與JPG輸入的問題ww</li>
</ul>
<h3 id="Lac10-Model-Attack"><a href="#Lac10-Model-Attack" class="headerlink" title="Lac10 : Model Attack"></a>Lac10 : Model Attack</h3><ul>
<li>在圖片加入一點點雜訊，讓圖片難以識別</li>
<li>介紹攻擊技術與防禦可能性</li>
</ul>
<h3 id="Lac11-Domain-Adaptation"><a href="#Lac11-Domain-Adaptation" class="headerlink" title="Lac11 : Domain Adaptation"></a>Lac11 : Domain Adaptation</h3><ul>
<li>當訓練資料與測試資料的分布不相似</li>
<li>EX. 手寫數字辨識，黑白與彩色的差別</li>
</ul>
<h3 id="Lac13-Network-Compression-模型壓縮"><a href="#Lac13-Network-Compression-模型壓縮" class="headerlink" title="Lac13 : Network Compression(模型壓縮)"></a>Lac13 : Network Compression(模型壓縮)</h3><ul>
<li>在嵌入式、小硬體跑ML</li>
</ul>
<h3 id="Lac14-Life-long-Learning"><a href="#Lac14-Life-long-Learning" class="headerlink" title="Lac14 : Life-long Learning"></a>Lac14 : Life-long Learning</h3><ul>
<li>機器能像人一樣一直學習所有技術</li>
<li>挑戰有哪些</li>
</ul>
<h3 id="Lac15-Meta-Learning-學習如何學習"><a href="#Lac15-Meta-Learning-學習如何學習" class="headerlink" title="Lac15 : Meta Learning(學習如何學習)"></a>Lac15 : Meta Learning(學習如何學習)</h3><ul>
<li>以往都是人設計學習演算法給機器</li>
<li>讓機器從大量學習任務裡面發明更好的演算法</li>
<li>Few-shot Learning通常與他相關</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/04/ML-2021-1-1-%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E6%A6%82%E8%AB%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/04/ML-2021-1-1-%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E6%A6%82%E8%AB%96/" class="post-title-link" itemprop="url">ML_2021_1-1 監督式學習概論</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-04 22:15:57" itemprop="dateCreated datePublished" datetime="2022-07-04T22:15:57+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-08 20:10:07" itemprop="dateModified" datetime="2022-08-08T20:10:07+08:00">2022-08-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="前言：什麼是機器學習"><a href="#前言：什麼是機器學習" class="headerlink" title="前言：什麼是機器學習"></a>前言：什麼是機器學習</h1><p>機器學習，就是用機器的力量幫忙找出一個合適的函數</p>
<ul>
<li>函數的輸入可以是vector、matrix、sequence</li>
<li>輸出可以是數值(regression)、類別(classification)、文章、圖片<br>深度學習，就是用神經網路的方法來製造函數</li>
</ul>
<h1 id="預測頻道觀看人數"><a href="#預測頻道觀看人數" class="headerlink" title="預測頻道觀看人數"></a>預測頻道觀看人數</h1><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><ul>
<li>loss function的輸入是weight跟bias，輸出是這組參數有多好」</li>
<li>Optimization : 找出最好的一組參數(w*,b*)使loss function最小<ul>
<li>常用gradient descent<ul>
<li>隨機決定w_0</li>
<li>把L對w做偏微分，若斜率&gt;0則提高w，反之降低w(變動量&#x3D;η，屬於hyperparameters)</li>
</ul>
</li>
</ul>
</li>
</ul>
<details>
<summary> step1: 製作一個學習函數 </summary>

<h2 id="純粹Linear-Model"><a href="#純粹Linear-Model" class="headerlink" title="純粹Linear Model"></a>純粹Linear Model</h2><ol>
<li><p>先隨便設一個函數<br>  $$f &#x3D; w_0 x_1$$<br>  其中$x_1$是昨天的觀看人數</p>
</li>
<li><p>根據loss function的結果嘗試把$w$、$b$優化，使得$f &#x3D; w_0 x_1 + b$有最小損失函數(這裡優化方法採梯度下降)</p>
</li>
<li><p>跑測試結果發現$w$很接近1 (看似很合理，因為昨天的頻道觀看人數與今天的頻道觀看人數差距應該不多)，其實預測都是前一天的結果平移而已，命中率有限 -&gt; 試試看一次看多幾天，變成以周為單位</p>
</li>
<li><p>修改函數<br>  $$f_1 \leftarrow y &#x3D; b+ \sum_{j&#x3D;1}^7 w_j x_j$$<br>  表示預測一天的觀看人數，需要前面七天觀看人數作為參考</p>
</li>
<li><p>命中率有效提高了（這是linear model）</p>
</li>
</ol>
<h2 id="Piecewise-Linear-Curve"><a href="#Piecewise-Linear-Curve" class="headerlink" title="Piecewise Linear Curve"></a>Piecewise Linear Curve</h2><ul>
<li>linear model過於簡單，x與y之間的關係是直線(w改動斜率，b改動原點)</li>
<li>利用一系列的線性函數相加，使加總後的函數有彎折</li>
<li>Piecewise Linear Curve &#x3D; constant + sum of a set of 藍線 (下圖紅線就是我們想求的「預測模型」函數f )<br>  <img src="/../images/20220701_1.png"></li>
<li>假設有無窮多個藍色function，就可以塑造出任意形狀的紅色function，如上圖所示</li>
</ul>
<h3 id="當特徵-x3D-1"><a href="#當特徵-x3D-1" class="headerlink" title="當特徵&#x3D;1"></a>當特徵&#x3D;1</h3><ul>
<li>如何寫出藍線式子?<ul>
<li>藍線函數令為 $f_{blue1} \leftarrow y &#x3D; c_i sigmoid(b_i+w_ix_1)$</li>
<li>使用sigmoid，把藍線變成曲型來逼近原型</li>
<li>原型稱為hard sigmoid</li>
<li>讓藍線凸在對的地方-&gt;修改w,b,c</li>
</ul>
</li>
<li>綜合前面的內容，我們就可以求得單一feature下的紅色function(而這樣理論上可以逼近任何連續函數)：<br>  $$f_{red1} \leftarrow y &#x3D; b + \sum_{i}c_i sigmoid(b_i + w_ix_1)$$</li>
<li>其中b也可以根據不同藍色function有所不同，把b丟入$\sum$即可</li>
</ul>
<h3 id="當特徵-gt-1"><a href="#當特徵-gt-1" class="headerlink" title="當特徵&gt;1"></a>當特徵&gt;1</h3><ul>
<li><p>當然，我們也可以進一步推廣，增加feature的量(目前只有一個，$x_1$的某特徵乘上各藍線的$w_i$)</p>
</li>
<li><p>解法就是修改藍線函數變成$f_{blue2} \leftarrow y &#x3D; c_i sigmoid(b_i + \sum_{j} w_{ij}x_j)$<br>  <img src="/../images/20220701_2.png" alt="upload successful"></p>
</li>
<li><p>同理把藍色函數相加再補上常數，紅線函數則可令為<br>$$f_{red2} \leftarrow y &#x3D; b + \sum_{i}c_i sigmoid(b_i + \sum_jw_{ij}x_j) $$</p>
</li>
<li><p>$w_{ij}$表示第i個sigmoid函數(aka第i條藍線)中，對於第$x_j$個特徵的權重</p>
<details>
<summary> 用線性代數的方式來理解 </summary>

<ul>
<li>繼續剛剛特徵&gt;1的討論</li>
<li>令$r_i &#x3D; b_i + \sum_{j}w_{ij}x_j$ (換句話說r就是sigmoid函數內的運算結果)，則可以把這個等式簡化為一個矩陣相乘<br>函數的示意圖如下:<br><img src="/../images/20220701_4.png" alt="upload successful"></li>
<li>而$r_i$做sigmoid()以後就會得到$a_i$</li>
<li>又稱$a &#x3D; \sigma(r)$</li>
<li>最後把$a_i$乘上各自的$c_i$後，加總得到剛剛的$f_{red2}$ (c在相乘時是$c^T$矩陣)</li>
<li>$b、c^T、b_i、W、x$五個向量append起來以後統稱為$\theta$</li>
</ul>
<p><img src="/../images/20220701_5.png" alt="upload successful"></p>
</details></li>
</ul>
  </details>

  <details>
  <summary> step2: 定義訓練資料的損失函數 </summary>

<ul>
<li>Loss is a function of parameters $L(\theta)$ ($\theta$就是step1中的那個)</li>
<li>就是x丟進去，測量y-hat跟y的差別多大  </details></li>
</ul>
  <details>
  <summary> step3: 最佳化模型 </summary>

<ul>
<li>假設最佳化的參數向量是$\theta^*$</li>
<li>則$\theta^* &#x3D; arg$ $min_{\theta}L$</li>
<li>首先，隨機選一個$\theta^0$作為初始值</li>
<li>Gradient descent作法<br>  $$<br>  g^T &#x3D;<br>  \begin{bmatrix}<br> \frac{\partial L}{\partial \theta_1}\<br> \frac{\partial L}{\partial \theta_2}\<br> \frac{\partial L}{\partial \theta_3}\<br> \ldots<br>  \end{bmatrix} |_{\theta &#x3D; \theta_0}<br>  $$<ul>
<li>註：打不出1xn向量&#x3D; &#x3D;</li>
</ul>
</li>
<li>則稱 g &#x3D; $\nabla L(\theta^0)$</li>
</ul>
<p>  求出g向量以後，就可以把它拿來更新參數列:<br>  <br><img src="/../images/pasted-0.png"></p>
<ul>
<li>亦即 $ \theta_1 \leftarrow \theta_0 - \eta g $</li>
</ul>
<h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ol>
<li>實務上通常做gradient不是所有資料都加入去更新，而是先把資料切成batch分別計算出$L^i$</li>
</ol>
<p><img src="/../images/20220702_1.png" alt="upload successful"></p>
<ul>
<li>一次的更新$\theta$是一次update，一次看完所有batch算一次epoch<ul>
<li>所以一個epoch會有好幾次update</li>
</ul>
</li>
</ul>
<ol start="2">
<li>Hard sigmoid也可以用兩個ReLU做出來</li>
</ol>
<p><img src="/../images/20220702_2.png" alt="upload successful"></p>
<ul>
<li>當然因為是要用兩個ReLU做出來，所以相應的紅線函數就要改<br>$$<br> f_{red_{ReLU}} \leftarrow y &#x3D; \sum_{i}c_i max(0,b_i + \sum_j w_{ij}x_j)<br>$$</li>
<li>這種做法比sigmoid好(下周講解)</li>
<li>ReLU跟sigmoid同為activation function  </details>

<details></li>
</ul>
<p>  <summary> step4: 繼續修改模型 </summary></p>
<p>  - 同樣的求a過程，我們可以做好幾次</p>
<p><img src="/../images/20220702_3.png" alt="upload successful"><br>    注意，圖中的W’、W、b’、b互不相同</p>
<p>  - 要做幾層同屬hyper parameter<br>  </details><br>  名詞解釋<br>  <br><img src="/../images/20220702_4.png" alt="upload successful"></p>
<!-- Preclass結束-->
</details>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://laxiflora.github.io/2022/07/04/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E9%A6%96%E9%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="劉宇承">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | laxiflora的小天地">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/04/%E6%9D%8E%E5%AE%8F%E6%AF%85-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E9%A6%96%E9%A0%81/" class="post-title-link" itemprop="url">李宏毅_機器學習_首頁</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">發表於</span>

      <time title="創建時間：2022-07-04 22:13:53" itemprop="dateCreated datePublished" datetime="2022-07-04T22:13:53+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新於</span>
      <time title="修改時間：2022-08-17 19:31:50" itemprop="dateModified" datetime="2022-08-17T19:31:50+08:00">2022-08-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="機器學習總編首頁"><a href="#機器學習總編首頁" class="headerlink" title="機器學習總編首頁"></a>機器學習總編首頁</h1><p>台大李弘毅老師的課</p>
<ul>
<li>課程影片： <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Ye018rCVvOo&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J">2021預習影片</a>  <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=7XZR0-4uS5s&list=PLJV_el3uVTsPM2mM-OQzJXziCGJa8nJL8">2022正課影片</a></li>
<li><a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php">課程網頁</a></li>
</ul>
<h1 id="2021年預習筆記"><a href="#2021年預習筆記" class="headerlink" title="2021年預習筆記"></a>2021年預習筆記</h1><h2 id="Ch-1：Introduction-of-Deep-Learning"><a href="#Ch-1：Introduction-of-Deep-Learning" class="headerlink" title="Ch 1：Introduction of Deep Learning"></a>Ch 1：Introduction of Deep Learning</h2><p>[1-1]<a href="/2022/07/04/ML-2021-1-1-%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E6%A6%82%E8%AB%96/">監督式學習概論</a></p>
<ul>
<li>介紹甚麼是機器學習，以及機器學習的任務</li>
<li>以預測李宏毅老師的頻道觀看人數為例，介紹監督式學習的運作流程<blockquote>
<p>關鍵字：<br>Linear model、Piecewise Linear Curve、sigmoid介紹、ReLU介紹、theta、Batch、Epoch</p>
</blockquote>
</li>
</ul>
<hr>
<h2 id="Ch-2：What-to-do-if-my-network-fails-to-train"><a href="#Ch-2：What-to-do-if-my-network-fails-to-train" class="headerlink" title="Ch 2：What to do if my network fails to train"></a>Ch 2：What to do if my network fails to train</h2><p>[2-1]<a href="/2022/07/04/ML-2021-2-1-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E4%BB%BB%E5%8B%99%E6%94%BB%E7%95%A5/">機器學習任務攻略</a></p>
<ul>
<li>機器學習上可能碰到的疑難雜症介紹，以及如何改善</li>
<li>區分命中率問題到底是overfitting、model bias issue還是Optimization issue，甚至是data mismatch</li>
<li>當一個模型單次測試test data成績很好，就保證是模型很好嗎？ 如何更確保模型真的很好呢？<blockquote>
<p>關鍵字：<br>Overfitting , model bias issue , Optimization issue之分辨、N-fold Cross Validation、data mismatch</p>
</blockquote>
</li>
</ul>
<hr>
<p>[2-2]<a href="/2022/07/04/ML-2021-2-2-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%B8%80/">類神經網路訓練不起來怎麼辦(一)</a></p>
<ul>
<li>續2-1的大綱，專注在討論optimizer失靈的解決方法與成因分辨</li>
<li>提供一個應用線性代數的例子：如何讓一個卡在鞍點的$\theta$逃離鞍點 (但計算曠日廢時)<ul>
<li>loss function &#x3D; MSE , optimizer &#x3D; 梯度下降法</li>
</ul>
</li>
<li>絕大多數的模型其實若卡在critical point，都是卡在鞍點而非局部最小值<blockquote>
<p>關鍵字：<br>Optimization issue、critical point、saddle point、local minima、<del>三體III：死神永生</del></p>
</blockquote>
</li>
</ul>
<hr>
<p>[2-3]<a href="/2022/07/05/ML-2021-2-3-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%BA%8C/">類神經網路訓練不起來怎麼辦(二)</a></p>
<ul>
<li>續2021版1-1的內容，我們這次探討batch size對於training與testing有甚麼影響</li>
<li>big batch size好還是small batch size好？ 要看hyper para怎麼調以及對於訓練速度的需求</li>
<li>額外介紹另一個對抗saddle point或local minima的技術 : momentum(慣性)<blockquote>
<p>關鍵字：<br>batch size、momentum</p>
</blockquote>
</li>
</ul>
<hr>
<p>[2-4]<a href="/2022/07/06/ML-2021-2-4-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%B8%89/">類神經網路訓練不起來怎麼辦(三)</a></p>
<ul>
<li>這次討論Optimizer中，關於Learning rate的問題</li>
<li>一個模型的跑動，常常碰到單一learning rate所會碰到的困境，所以我們需要可以隨著單次訓練中各種情況變動的learning rate</li>
<li>提供兩種改變learning rate的作法以及相關參數計算</li>
<li>簡單介紹這些手法被現今哪些熱門優化器使用<blockquote>
<p>關鍵字:<br>Learning rate($\eta$)、Adaptive learning rate、Adam-learning rate、RMSProp、Warm up(learning rate)、learning rate decay</p>
</blockquote>
</li>
</ul>
<hr>
<p>[2-5]<a href="/2022/07/06/ML-2021-2-5-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E5%9B%9B/">類神經網路訓練不起來怎麼辦(四)</a></p>
<ul>
<li>簡短介紹當碰到分類問題的時候，最後一層激發函數該選哪個函數</li>
<li>介紹了one-hot vector作為新的輸出型態，以及基本原因</li>
<li>介紹分類模型的常用loss function，以及用optimizer的角度看兩種loss function的差距</li>
<li>因為是簡短版，內附有<del>冗長版</del> 完整版連結<blockquote>
<p>關鍵字:<br>one-hot vector、Softmax、cross entropy</p>
</blockquote>
</li>
</ul>
<hr>
<p>[2-6]<a href="/2022/07/07/ML-2021-2-6-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%BA%94/">類神經網路訓練不起來怎麼辦(五)</a></p>
<ul>
<li>簡短介紹Batch Normalization的技術</li>
<li>這是另一種直接改變error surface的技術(相對於動態lr)</li>
<li>關於batch normalization為何能夠讓模型訓練更好仍是個謎<blockquote>
<p>關鍵字：<br>Feature Normalization、Batch Normalization</p>
</blockquote>
</li>
</ul>
<hr>
<h2 id="Ch3-image-as-input"><a href="#Ch3-image-as-input" class="headerlink" title="Ch3 : image as input"></a>Ch3 : image as input</h2><p>[3-1]<a href="/2022/07/09/ML-2021-3-1-%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF/">卷積神經網路</a></p>
<ul>
<li>介紹CNN neurol network 以及他的相關常用術語</li>
<li>旨在讓我們了解CNN，一個入門指引</li>
<li>以CNN：apply in image為例，用於文字辨識或是語音辨識需要更多參考相關文獻，用於影像的CNN不一定適用其他輸入<blockquote>
<p>關鍵字：<br>Receptive Field(Kernel size)、Filter、Parameter sharing、Pattern、Stride、Convolusion、Feature Map、Subsample (pooling)、padding(pads)、channel、(Rescale、data augmentation)</p>
</blockquote>
</li>
</ul>
<hr>
<h2 id="Ch4：Sequence-as-input"><a href="#Ch4：Sequence-as-input" class="headerlink" title="Ch4：Sequence as input"></a>Ch4：Sequence as input</h2><p>[4-1]<a href="/2022/07/13/ML-2021-4-1-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%EF%BC%88%E4%B8%8A%EF%BC%89/">自注意力機制（上）</a></p>
<ul>
<li>介紹在碰到輸入的時候不只是一個向量，甚至可能輸入的向量數量會變動，該怎麼處理</li>
<li>簡單介紹輸入為sequence的常見問題以及不同輸出的類別</li>
<li>介紹如果輸入的sequence，不同vector之間互相影響該怎麽處理</li>
<li>本課程著重在討論sequence中每個vector都會有獨立一個label輸出的情況<blockquote>
<p>關鍵字：<br>Sequence、word embedding、window(語音處理)、self-attention、transformer(提到)、seq2seq（提到）</p>
</blockquote>
</li>
</ul>
<hr>
<p>[4-2]<a href="/2022/07/13/ML-2021-4-2-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6%EF%BC%88%E4%B8%8B%EF%BC%89/">自注意力機制（下）</a></p>
<ul>
<li>常用(上週討論的)self-attention計算方法，其實就是很多的矩陣相乘</li>
<li>當一個vector出現在sequence的不同位置也需要考量，該怎麼處理 -&gt; Positional encoding</li>
<li>因為attention matrix之空間大小是$\theta(n^2)$，所以當sequence很大(ex.音訊)時，該怎麼處理 -&gt; Truncated self-attention</li>
<li>self-attention可否用於影像呢？ 如果這樣使用的話，self-attention跟CNN差別在哪？誰優誰劣呢</li>
<li>Self-attention與RNN之間的差別在哪</li>
<li>self-attention如何使用在graph中呢？GNN是什麼呢（refrence）</li>
<li>Reference各種transformer的變形(survey paper)<br>PS. 這塊領域目前很新，論文大多產自2019&#x2F;2020年<blockquote>
<p>關鍵字：<br>Transformer(self-attention塊)、positional encoding、truncation self-attention、v.s CNN、v.s RNN、GNN</p>
</blockquote>
</li>
</ul>
<hr>
<h2 id="Ch5：Sequence-2-Sequence"><a href="#Ch5：Sequence-2-Sequence" class="headerlink" title="Ch5：Sequence 2 Sequence"></a>Ch5：Sequence 2 Sequence</h2><ul>
<li>算是Ch4中的一種特例輸入</li>
<li>5-1就是2-6，介紹batch norm</li>
</ul>
<hr>
<p>[5-2]<a href="/2022/07/23/ML-2021-5-2-Transformer%EF%BC%88%E4%B8%8A%EF%BC%89/">Transformer(上)</a></p>
<ul>
<li>首先講解seq2seq類模型所可以解決的問題與應用，接下來講解seq2seq的其中一種模型「transformer」</li>
<li>介紹transformer的大架構圖，以及transformer encoder的分解步驟</li>
<li>除了原始的transformer encoder模型圖以外，還有reference其他種transform encoder的架構</li>
<li>BERT其實就是transformer的encoder<blockquote>
<p>關鍵字:<br>Transformer(總覽 &amp; encoder)、seq2seq類應用</p>
</blockquote>
</li>
</ul>
<hr>
<p>[5-3]<a href="/2022/07/24/ML-2021-5-3-Transformer%EF%BC%88%E4%B8%8B%EF%BC%89/">Transformer（下）</a></p>
<ul>
<li>接續5-2，介紹原始Transformer decoder之詳細步驟，以及計算loss的方式</li>
<li>介紹為何transformer的loss function要用cross entropy，而不能直接用如BLEU等等判別算法，並說明他們兩者之間並不一定正相關</li>
<li>Transformer依decoder訓練方式有分為AT與NAT，本課主要講姊AT</li>
<li>介紹Seq2Seq常見的一些training技巧與問題</li>
</ul>
<blockquote>
<p>關鍵字:<br>  Transformer(decoder、loss)、AT &amp; NAT、Scheduled Sampling、Beam search、Teacher forcing、Copy mechanism、Guided Attention</p>
</blockquote>
<hr>
<h2 id="Ch6：Generation-Generative-Adversarial-Network-GAN"><a href="#Ch6：Generation-Generative-Adversarial-Network-GAN" class="headerlink" title="Ch6：Generation  (Generative Adversarial Network, GAN)"></a>Ch6：Generation  (Generative Adversarial Network, GAN)</h2><ul>
<li><a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2022-course-data/Machine%20Learning%20HW6.pdf">PPT連結</a></li>
</ul>
<p>[6-1]<a href="/2022/07/27/ML-2021-6-1-%E7%94%9F%E6%88%90%E5%BC%8F%E5%B0%8D%E6%8A%97%E7%B6%B2%E8%B7%AF-%E4%B8%80-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%B4%B9/">生成式對抗網路(一) – 基本概念介紹</a></p>
<ul>
<li>介紹GAN的訓練過程、用途、基本原理</li>
<li>GAN變種非常多</li>
<li><del>這集超油</del></li>
</ul>
<blockquote>
<p>關鍵字：<br>  discriminator、generator、Train GAN process、GAN zoo</p>
</blockquote>
<hr>
<p>[6-2]<a href="/2022/07/28/ML-2021-6-2-%E7%94%9F%E6%88%90%E5%BC%8F%E5%B0%8D%E6%8A%97%E7%B6%B2%E8%B7%AF-%E4%BA%8C-%E7%90%86%E8%AB%96%E4%BB%8B%E7%B4%B9%E8%88%87WGAN/">生成式對抗網路(二) – 理論介紹與WGAN</a></p>
<ul>
<li>介紹Discriminator與Generator如何訓練</li>
<li>true data跟generated data之distribution差距過大，原本的JS GAN discriminator很難evaluate</li>
<li>WGAN的discriminator算法與reference<br>Note：這部看不是很懂，要複習</li>
</ul>
<blockquote>
<p>關鍵字：<br>  WGAN、Wasserstein distance、1-Lipschitz function(待查)</p>
</blockquote>
<hr>
<p>[6-3]<a href="/2022/07/29/ML-2021-6-3-%E7%94%9F%E6%88%90%E5%BC%8F%E5%B0%8D%E6%8A%97%E7%B6%B2%E8%B7%AF-%E4%B8%89-%E2%80%93-%E7%94%9F%E6%88%90%E5%99%A8%E6%95%88%E8%83%BD%E8%A9%95%E4%BC%B0%E8%88%87%E6%A2%9D%E4%BB%B6%E5%BC%8F%E7%94%9F%E6%88%90/">生成式對抗網路(三) – 生成器效能評估與條件式生成</a></p>
<ul>
<li>續6-2，貼了一些關於GAN training tips的延伸學習</li>
<li>GAN + Transformer (GAN in sequence generation)</li>
<li>除了GAN以外，還有哪些也是在做generation的model</li>
<li>GAN的優劣如何衡量？ 會有哪些問題被忽略？</li>
<li>Conditional GAN(把開頭GAN被拔掉的x放回來了)</li>
<li>Conditional GAN相關天馬行空的應用</li>
</ul>
<blockquote>
<p>關鍵字：<br>    VAE、FLOW-based Model、Mode Collapse、Mode Dropping、Inception score(IS)、Frechet inception distance(FID)、Conditional Generation</p>
</blockquote>
<hr>
<p>[6-4]<a href="/2022/07/29/ML-2021-6-4-%E7%94%9F%E6%88%90%E5%BC%8F%E5%B0%8D%E6%8A%97%E7%B6%B2%E8%B7%AF-%E5%9B%9B-%E2%80%93-Cycle-GAN/">生成式對抗網路(四) – Cycle GAN</a></p>
<ul>
<li>以影像風格轉換為例，說明cycle GAN如何應用在unsupervised learning上面</li>
<li>reference 可以接受更多風格的非監督式GAN</li>
<li>介紹其他更多任務的GAN unsupervised learning (撇除圖片)</li>
</ul>
<p>Note: 有一些地方觀念模糊，為何generator輸出是文字，丟給discriminator會出現問題？為何需要用RL？</p>
<blockquote>
<p>關鍵字：<br>  cycle GAN 、GAN unsupervised learning、seq2seq generator unsupervised learning</p>
</blockquote>
<hr>
<h2 id="Ch-6-5-：Recent-Advance-of-Self-supervised-learning-for-NLP"><a href="#Ch-6-5-：Recent-Advance-of-Self-supervised-learning-for-NLP" class="headerlink" title="Ch 6.5 ：Recent Advance of Self-supervised learning for NLP"></a>Ch 6.5 ：Recent Advance of Self-supervised learning for NLP</h2><p><strong>以BERT、GPT為例介紹近期self-supervised learning model的原理與在NLP上的應用</strong></p>
<p>[X-1 &amp; X-2]<a href="/2022/07/30/ML-2021-X-1-X-2-BERT%E7%B0%A1%E4%BB%8B/">自督導式學習(一、二) – BERT簡介</a></p>
<ul>
<li>簡介BERT的訓練(pretrain)方法，以及一些應用</li>
<li>how to fine-tune BERT in some cases.</li>
<li>帶一下pretrain decoder的方法</li>
</ul>
<blockquote>
<p>關鍵字：<br>  Fine-tune、pretrain (Next sentence prediction、sentence order prediction(SOP)、masking input)</p>
</blockquote>
<hr>
<p>[X-3]<a href="/2022/08/03/ML-2021-X-3-BERT%E7%9A%84%E5%A5%87%E8%81%9E%E8%BB%BC%E4%BA%8B/">自督導式學習(三) - BERT的奇聞軼事</a></p>
<ul>
<li><p>Why BERT works？ 簡單介紹word embedding</p>
</li>
<li><p>關於BERT的表現作了一些相關實驗</p>
<ol>
<li>BERT學習填空，是否真的是看得懂文章？ 如果輸入的sequence毫無邏輯，BERT是否受到影響？</li>
<li>如果BERT pretrain使用多種語言，是否可以用於解決新的語言的問題？</li>
</ol>
</li>
<li><p>對於BERT的猜想</p>
<blockquote>
<p>關鍵字：<br>  contextualized word embedding、Multi-lingual BERT</p>
</blockquote>
</li>
</ul>
<hr>
<p>[X-4]<a href="/2022/08/04/ML-2021-X-4-GPT%E7%9A%84%E9%87%8E%E6%9C%9B/">自督導式學習(四) – GPT的野望</a></p>
<ul>
<li>介紹另一個self-supervised learning model：GPT</li>
<li>GPT的任務目標比起BERT更加有野心：期望能夠輸入task description與問題，就能自己預測出答案</li>
<li>GPT的訓練方式類似transformer的decoder，給定一個seq，要能預測下一個token是甚麼<blockquote>
<p>關鍵字：<br>GPT</p>
</blockquote>
</li>
</ul>
<hr>
<h2 id="Ch8：Auto-encoder-x2F-Anomaly-Detection"><a href="#Ch8：Auto-encoder-x2F-Anomaly-Detection" class="headerlink" title="Ch8：Auto-encoder&#x2F; Anomaly Detection"></a>Ch8：Auto-encoder&#x2F; Anomaly Detection</h2><p>[8-1]<a href="/2022/08/08/ML-2021-8-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">自編碼器(上) – 基本概念</a></p>
<ul>
<li>介紹auto-encoder的模型，以及它的學習任務</li>
<li>auto-encoder跟現今cycle GAN的2 generator關係很像</li>
<li>介紹de-noising auto-encoder，並且分析它與現今的self-supervised learning model(BERT)的相似</li>
<li>auto-encoding 具備降維壓縮的功能</li>
</ul>
<blockquote>
<p>關鍵字<br>  de-noising auto-encoder、embedding(Representation, Code)、dimention reduction</p>
</blockquote>
<hr>
<p>[8-2]<a href="/2022/08/09/ML-2021-8-2-%E9%A0%98%E7%B5%90%E8%AE%8A%E8%81%B2%E5%99%A8%E8%88%87%E6%9B%B4%E5%A4%9A%E6%87%89%E7%94%A8/">自編碼器(下) – 領結變聲器與更多應用</a></p>
<ul>
<li>柯南的領結變聲器，就是一種voice conversion的應用。現實中要做到這一點，就需要對embedding有更多理解 – Feature Disentanglement</li>
<li>除了傳統的auto-encoder之外，還有各種auto-encoder的變形</li>
<li>embedding(representation)的各種花招以及模型修改</li>
<li>auto-encoder的更多應用，比如本次作業會用到的異常檢測</li>
</ul>
<blockquote>
<p>關鍵字<br>  Feature Disentanglement、Discrete Latent Representation、Text&#x2F;Tree as Representation、VAE、Anomaly Detection</p>
</blockquote>
<hr>
<p>Note:<br>8-3 ~ 8-8 是Anomaly Detection主題的內容<br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=gDp2LXGnVLQ&t=5s">第一部影片連結</a><br>這裡先略過</p>
<hr>
<h2 id="Ch9：Explainable-AI"><a href="#Ch9：Explainable-AI" class="headerlink" title="Ch9：Explainable AI"></a>Ch9：Explainable AI</h2><p>[9-1]<a href="/2022/08/09/ML-2021-9-1-%E7%82%BA%E4%BB%80%E9%BA%BC%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E5%8F%AF%E4%BB%A5%E6%AD%A3%E7%A2%BA%E5%88%86%E8%BE%A8%E5%AF%B6%E5%8F%AF%E5%A4%A2%E5%92%8C%E6%95%B8%E7%A2%BC%E5%AF%B6%E8%B2%9D%E5%91%A2/">機器學習模型的可解釋性(上) – 為什麼類神經網路可以正確分辨寶可夢和數碼寶貝呢？</a></p>
<ul>
<li>為何需要可解釋性的AI? 對於AI可解釋的標準定義是?</li>
<li>可解釋性AI的類型</li>
<li>要能找出一個輸入的哪個部位重要，有哪些技巧?</li>
<li>要能看出機器怎麼對輸入做處理，有那些技巧?</li>
</ul>
<blockquote>
<p>關鍵字：<br>  local&#x2F;global explanation、Saliency map、SmoothGrad、Visualization、Probing</p>
</blockquote>
<hr>
<p>[9-2]<a href="/2022/08/10/ML-2021-9-2-%E6%A9%9F%E5%99%A8%E5%BF%83%E4%B8%AD%E7%9A%84%E8%B2%93%E9%95%B7%E4%BB%80%E9%BA%BC%E6%A8%A3%E5%AD%90/">機器學習模型的可解釋性(下) – 機器心中的貓長什麼樣子？</a></p>
<ul>
<li>著重在global explainable的各種approach<ul>
<li>觀察convoluation layer output</li>
<li>看classifier output</li>
<li>做一個簡易版可解釋的model模仿他的行為</li>
</ul>
</li>
</ul>
<hr>
<h2 id="Ch10：Adversarial-Attack"><a href="#Ch10：Adversarial-Attack" class="headerlink" title="Ch10：Adversarial Attack"></a>Ch10：Adversarial Attack</h2><p>[10-1]<a href="/2022/08/11/ML-2021-10-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">來自人類的惡意攻擊(上) – 基本概念</a></p>
<ul>
<li>介紹攻擊的原理、作法之精神</li>
<li>常見attack相關限制的介紹與計算方式<ul>
<li>新圖與原圖的距離差距與計算方法</li>
</ul>
</li>
<li>實際攻擊的一種approach method</li>
</ul>
<blockquote>
<p>關鍵字：<br>  benign image&#x2F;attacked image、FGSM</p>
</blockquote>
<hr>
<p>[10-2]<a href="/2022/08/11/ML-2021-10-2-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%83%BD%E5%90%A6%E8%BA%B2%E9%81%8E%E4%BA%BA%E9%A1%9E%E6%B7%B1%E4%B8%8D%E8%A6%8B%E5%BA%95%E7%9A%84%E6%83%A1%E6%84%8F/">來自人類的惡意攻擊(下) – 類神經網路能否躲過人類深不見底的惡意？</a></p>
<ul>
<li>除了白箱攻擊以外，講解黑箱攻擊的技巧</li>
<li>簡單探討為何黑箱攻擊具有可行性 -&gt; 資料的特徵問題，而非模型?</li>
<li>可攻擊的領域與方法</li>
<li>如何防禦黑箱攻擊</li>
</ul>
<blockquote>
<p>關鍵字：<br>  Proxy network、Ensemble attack、One pixel attack、Universal Adversarial Attack、Adversarial reprogramming、Backdoor in model、被動：Randomization、主動：Adversarial training、、、、、、</p>
</blockquote>
<hr>
<h2 id="Ch11：Adaptation"><a href="#Ch11：Adaptation" class="headerlink" title="Ch11：Adaptation"></a>Ch11：Adaptation</h2><p>[11-1]<a href="/2022/08/15/ML-2021-11-1-%E6%A6%82%E8%BF%B0%E9%A0%98%E5%9F%9F%E8%87%AA%E9%81%A9%E6%87%89/">概述領域自適應</a></p>
<ul>
<li>概述何為Domain shift</li>
<li>Domain adaptation名詞解釋</li>
<li>根據condition的不同(target,source差距、手握的data量)，簡單講解domain adaptation的各種approach<ul>
<li>本課著重在target domain unlabeled data很多的前提</li>
</ul>
</li>
<li>提供各種condition的參考文獻（列入關鍵字）<blockquote>
<p>關鍵字：<br>source&#x2F;target domain、Domain Adversarial training、Feature Extractor、Domain Classifier、Label Predictor、Universal domain adaptation、Testing time training、Domain Generalization</p>
</blockquote>
</li>
</ul>
<hr>
<h2 id="CH12：Reinforcement-Learning"><a href="#CH12：Reinforcement-Learning" class="headerlink" title="CH12：Reinforcement Learning"></a>CH12：Reinforcement Learning</h2><p>[12-1]<a href="/2022/08/17/ML-2021-12-1-%E5%A2%9E%E5%BC%B7%E5%BC%8F%E5%AD%B8%E7%BF%92%E8%B7%9F%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E4%B8%80%E6%A8%A3%E9%83%BD%E6%98%AF%E4%B8%89%E5%80%8B%E6%AD%A5%E9%A9%9F/">概述增強式學習(一) – 增強式學習跟機器學習一樣都是三個步驟</a></p>
<hr>
<h1 id="2022年正課筆記"><a href="#2022年正課筆記" class="headerlink" title="2022年正課筆記"></a>2022年正課筆記</h1><p>[1-1]<a href="/2022/07/04/ML-2022-1-1-%E6%AD%A3%E8%AA%B2%E5%85%A7%E5%AE%B9%E4%BB%8B%E7%B4%B9/">正課內容介紹</a></p>
<ul>
<li>介紹各講的重點核心</li>
<li>$X-y$中的X對應的是第幾講，2021年的編號也同樣，是按造課程網頁的syllabus排的</li>
</ul>
<hr>
<p>[2-1]<a href="/2022/07/04/ML-2022-2-1-%E5%86%8D%E6%8E%A2%E5%AF%B6%E5%8F%AF%E5%A4%A2%E3%80%81%E6%95%B8%E7%A2%BC%E5%AF%B6%E8%B2%9D%E5%88%86%E9%A1%9E%E5%99%A8/">再探寶可夢、數碼寶貝分類器</a></p>
<hr>
<h1 id="作業區-有寫的部分orz"><a href="#作業區-有寫的部分orz" class="headerlink" title="作業區(有寫的部分orz)"></a>作業區(有寫的部分orz)</h1><p><a href="/2022/07/11/ML-LEE-2022-hw3/">作業3 - 圖像辨識，使用CNN</a></p>
<hr>
<p><a href="/2022/07/23/ML-LEE-2022-hw4/">作業4 - 語音辨識，使用Transformer</a></p>
<hr>
<p><a href="/2022/08/04/ML-LEE-2022-hw5/">作業5 - Transformer</a></p>
<hr>
<p><a href="/2022/08/04/ML-LEE-2022-hw6/">作業6 - GAN</a></p>
<hr>
<p><a href="/2022/08/04/ML-LEE-2022-hw7/">作業7 - BERT</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="上一頁"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="下一頁"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">劉宇承</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 強力驅動
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
